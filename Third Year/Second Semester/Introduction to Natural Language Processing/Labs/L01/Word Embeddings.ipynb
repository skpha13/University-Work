{"cells":[{"cell_type":"markdown","metadata":{"id":"jEuuzNUQ_MSG"},"source":["<!-- # Word Representations -->\n","# Reprezentarea Cuvintelor\n","\n","칉n procesarea limbajului natural, word embeddings (reprezentarea cuvintelor prin proiec탵ii) este termenul folosit pentru reprezentarea cuvintelor ca vectori. Pentru a antrena un model, avem nevoie de date numerice, deci avem nevoie de o modalitate prin care s캒 transform캒m un text 칥n vectori de numere 칥ncerc칙nd s캒 p캒str캒m c칙t mai multe informa탵ii relevante pentru ce vrem s캒 facem. A탳adar, uneori ne intereseaz캒 rela탵iile semantice, alteori con탵inutul lexical 탳amd.\n","\n","<!-- In natural language processing, word embedding is the term used for representing a word as a vector. For training a model we need numerical data, which means that we must find a way to represent texts such that we keep as much information as possible considering our current context. This means that sometimes semantic relations will be more important, other times lexical information etc. -->\n","\n","Prin vectorizarea cuvintelor, reprezent캒m fiecare cuv칙nt ca un num캒r sau ca o list캒 de numere. 칉n cazul reprezent캒rilor dense/continue ale cuvintelor, ideea este s캒 reprezent캒m cuvinte similare ca fiind apropiate 칥n spa탵iul vectorial 칥n care le proiect캒m.\n","\n","<!-- By using word embeddings (vectorization) we can represent each word as a number or a list of numbers that conveys this information such that words that are similar will be closer to each other in the vector space than words that are not. -->\n","\n","[word2vec ilustrat](https://jalammar.github.io/illustrated-word2vec/) (revenim la word2vec ora viitoare)"]},{"cell_type":"markdown","metadata":{"id":"sUqzZuWWhUuw"},"source":["# Bag of Words (BoW) / Sac de cuvinte\n","\n","Pentru situa탵iile c칙nd contextul 탳i ordinea cuvintelor nu este relevant캒, ci doar c칙t de des apar cuvintele, atunci am folosi BoW. E ca 탳i cum am \"arunca\" toate cuvintele 칥ntr-un sac (eventual le 탳i amestec캒m) iar apoi num캒r캒m de c칙te ori apare fiecare cuv칙nt (un fel de vector de frecven탵캒). Ca un caz particular, putem avea BoW binar (un fel de vector de apari탵ii). Aceasta este cam cea mai simpl캒 tehnic캒 de vectorizat un text.\n","\n","<!-- Imagine a situation where the context of the words is not relevant, only how often they appear. This is where we use bag of words. This approach just throws all words in a bag, maybe shuffles it a bit, then counts how many times each words appears (or if they appear in case of a binary BoW). It is the easiest vectorization method that we will discuss. -->"]},{"cell_type":"markdown","metadata":{"id":"IS2s198nbhgT"},"source":["<center><img src='https://drive.google.com/uc?export=view&id=1v6McR199QkVXvuQmC3FWJ80rSXGTbZUS' width=500></center>"]},{"cell_type":"markdown","metadata":{"id":"3AExe5lbiUNm"},"source":["S캒 lu캒m exemplul din imagine dat ca exemplu. Fie scriem noi implementarea de la zero, fie folosim [CountVectorizer](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html) gata implementat 칥n scikit-learn:\n","\n","<!-- Let's take the text from the example. We can either write our own BoW implementation, or we can use the one preimplemented in [CountVectorizer](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html): -->"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":264,"status":"ok","timestamp":1698067653684,"user":{"displayName":"MIRUNA-ANDREEA ZAVELCA","userId":"12692883064275182332"},"user_tz":-180},"id":"ymHBxvsQiKXV","outputId":"31f8c0fe-ec94-4286-e82c-ba2a24217bd2"},"outputs":[{"name":"stdout","output_type":"stream","text":["['did' 'fly' 'see' 'the' 'will' 'with' 'you']\n","[[1 1 1 1 0 0 1]\n"," [0 2 0 1 1 1 1]]\n"]}],"source":["from sklearn.feature_extraction.text import CountVectorizer\n","\n","text = ['Did you see the fly?', 'The fly will fly with you.']\n","vectorizer = CountVectorizer()\n","X = vectorizer.fit_transform(text)\n","print(vectorizer.get_feature_names_out())\n","print(X.toarray())"]},{"cell_type":"markdown","metadata":{"id":"RGKVdy4-sgoy"},"source":["`CountVectorizer` are un constructor cu mul탵i parametri implici탵i pe care 칥i putem suprascrie. De exemplu, dac캒 vrem doar reprezentare binar캒 (vector de apari탵ii) 탳i s캒 num캒r캒m doar bigrame, am proceda astfel:\n","\n","<!-- CountVectorizer is a class with predefined parameters. You can always change those parameters, meaning that you can, for example, choose to have a binary representation of bigrams: -->"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":651,"status":"ok","timestamp":1698067678802,"user":{"displayName":"MIRUNA-ANDREEA ZAVELCA","userId":"12692883064275182332"},"user_tz":-180},"id":"8n3BFJtLpb2H","outputId":"223bcc80-aac6-4250-a68c-bd2f856b2604"},"outputs":[{"name":"stdout","output_type":"stream","text":["['am not' 'he is' 'is very' 'not happy' 'very happy']\n","[[1 0 0 1 0]\n"," [0 1 1 0 1]]\n"]}],"source":["text = ['I am not happy.', 'He is very happy']\n","vectorizer = CountVectorizer(binary=True, ngram_range=(2, 2))\n","X = vectorizer.fit_transform(text)\n","print(vectorizer.get_feature_names_out())\n","print(X.toarray())"]},{"cell_type":"markdown","metadata":{"id":"ThLRQJI3uLyc"},"source":["N-gramele (la nivel de cuv칙nt) sunt secven탵e de n cuvinte. Ele sunt utile pentru a furniza context, de exemplu pentru a diferen탵ia 칥ntre _not happy_ 탳i _verry happy_. Le putem folosi fie ca features (reprezent캒ri numerice), fie ca s캒 analiz캒m setul de date.\n","\n","<!-- N-grams are sequences of n words. They help us get some context about the text, letting us know the difference between _not happy_ and _very happy_ for example. This can be used as a feature for another representation, or on its own to make assumptions about the dataset. -->"]},{"cell_type":"markdown","metadata":{"id":"yLcyp5Ed6QKT"},"source":["#  Term Frequency - Inverse Document Frequency (Tf-idf)\n","\n","Doar pentru c캒 un cuv칙nt apare de multe ori, nu 칥nseamn캒 c캒 este 탳i relevant dpdv al con탵inutului (vezi stopwords). De asemenea, stopwords variaz캒 de la un domeniu la altul 탳i poate fi anevoios s캒 ne definim manual de fiecare dat캒 liste de stopwords. Dac캒 de exemplu vrem s캒 ne facem un motor de c캒utare, ar fi mai relevant un cuv칙nt care apare des 칥ntr-un document, 칥ns캒 nu este frecvent 칥nt칙lnit 칥n celelalte documente (cum se 칥nt칙mpl캒 칥n cazul stopwords).\n","\n","<!-- Just because a word appears often it does not mean that it is necessarily relevant (think about stopwords). If we want to write a search engine for example, it would be more relevant for us to know how often a certain word appears in a document with regards to how common that word generally is. Tf-idf is an algorithm that takes this into account. In other words, a word is important for a given document if it appears many times in this one and rarely in others. -->"]},{"cell_type":"markdown","metadata":{"id":"KaNvpl8r1teo"},"source":["Pentru un document dat, repet캒m urm캒toarele opera탵ii pentru fiecare cuv칙nt din 칥ntreg setul de date:\n","\n","<!-- We will consider the given document as the current datapoint and repeat the following for each word in the dataset: -->\n","\n","$$TFIDF = TF * IDF$$\n","unde:\n","<!-- $$TF(word, document) = \\frac{How\\ many\\ times\\ the\\ word\\ appears\\ in\\ the\\ document}{Number\\ of\\ words\\ in\\ the\\ document}$$ -->\n","$$TF(cuv칙nt, document) = \\frac{\\text{#apari탵ii ale cuv칙ntului 칥n document}}{\\text{# de cuvinte din document}}$$\n","탳i:\n","$$IDF(cuv칙nt, Documente) = log(\\frac{\\text{# de documente din corpus}}{1 + \\text{# de documente care con탵in cuv칙ntul curent}} + 1)$$\n","\n","<!-- We use **log** in order to smooth our values for an easier analysis. -->\n","Folosim **log** pentru a normaliza valorile 탳i pentru a fi mai u탳or de analizat."]},{"cell_type":"markdown","metadata":{"id":"VDamrGeM6H5I"},"source":["Pentru implementare, avem [TfidfVectorizer](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html). Reprezentarea va fi o matrice unde fiecare r칙nd corespunde unui document 탳i fiecare coloan캒 corespunde unui cuv칙nt din tot setul de date:\n","\n","<!-- For the implementation you can use [TfidfVectorizer](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html). The output will be a matrix where each row corresponds to a datapoint and each column to a word from the full dataset: -->"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":408,"status":"ok","timestamp":1698087478538,"user":{"displayName":"MIRUNA-ANDREEA ZAVELCA","userId":"12692883064275182332"},"user_tz":-180},"id":"PA5hy4bW4d78","outputId":"8155320d-476e-4dc6-d653-99a620cadc31"},"outputs":[{"name":"stdout","output_type":"stream","text":["['am' 'happy' 'he' 'is' 'not' 'very']\n","[[0.6316672  0.44943642 0.         0.         0.6316672  0.        ]\n"," [0.         0.37997836 0.53404633 0.53404633 0.         0.53404633]]\n"]}],"source":["from sklearn.feature_extraction.text import TfidfVectorizer\n","\n","text = ['I am not happy.', 'He is very happy']\n","\n","vectorizer = TfidfVectorizer()\n","X = vectorizer.fit_transform(text)\n","print(vectorizer.get_feature_names_out())\n","print(X.toarray())"]},{"cell_type":"markdown","source":["# Reprezent캒ri continue ale cuvintelor\n","\n","Reprezent캒rile de p칙n캒 acum (BoW, Tf-Idf) au ca dezavantaje\n","- necesitatea unui num캒r extrem de mare de elemente pe m캒sur캒 ce avem texte de dimensiuni mai mari\n","- nu au un mod de a reprezenta rela탵iile semantice dintre cuvinte\n","\n","Pentru a ocupa mai pu탵in캒 memorie, se folosesc reprezent캒ri cu matrici rare (sparse matrix). Totu탳i, chiar 탳i a탳a, aceste reprezent캒ri rare ale cuvintelor nu scaleaz캒 c칙nd dimensiunea vocabularului este foarte mare.\n","\n","Ca solu탵ii la aceast캒 problem캒, au ap캒rut reprezent캒rile dense (continue) de cuvinte, unde num캒rul de dimensiuni ale proiec탵iilor cuvintelor este mult mai mic.\n","\n","Dimensiunea unui vocabular poate ajunge la c칙teva zeci sau sute de mii de cuvinte (탳i chiar milioane), deci utilizarea unor matrice de astfel de dimensiuni 칥n re탵ele neurale devine extrem de costisitoare.\n","\n","### Pe scurt despre word2vec (2013) 탳i GloVe (2014)\n","\n","Reprezent캒rile continue sunt pe scurt o matrice utilizat캒 pentru a reduce dimensiunea unui vector. Dac캒 dimensiunea vocabularului este V 탳i reducem la ni탳te embeddings (reprezent캒ri/proiec탵ii) de dimensiune N, avem o matrice de $V*N$. Ini탵ial, din textul nostru vom reprezenta un cuv칙nt ca un vector one-hot de dimensiune $V$.\n","\n","Pentru a ob탵ine reprezentarea cuv칙ntului, 칥nmul탵im vectorul cu matricea de proiec탵ie (embedding matrix), deci ob탵inem un vector de dimensiune $N$.\n","\n","De exemplu, dimensiunea vocabularului este de $10^5$ 탳i reducem la ni탳te reprezent캒ri de dimensiune 300. Astfel, 칥n loc s캒 lucr캒m cu vectori de dimensiune $10^5$ 칥n re탵elele noastre, avem doar vectori de dimensiuni 300, mult mai mici.\n","\n","### fastText\n","\n","Cu toate c캒 reprezent캒rile word2vec 탳i GloVE ne rezolv캒 problemele cu reprezent캒rile rare ale cuvintelor, acestea au o limitare (existent캒 탳i 칥nainte) referitoare la modul 칥n care ar trebui tratate cuvintele care nu exist캒 칥n setul de date ini탵ial.\n","\n","Cuvintele din afara vocabularului (OOVW - out of vocabulary words) nu pot fi reprezentate 칥n spa탵iul vectorial dat de matricea de proiec탵ie, deoarece nu apar 칥n vocabular. C칙teva metode de remediere:\n","- cel mai simplu, ad캒ug캒m 칥n vocabular un cuv칙nd UNK (unknown - necunoscut)\n","- folosim stemming sau lematizare 칥n speran탵a c캒 vom g캒si un cuv칙nt similar 칥n vocabularul ini탵ial\n","- c캒ut캒m sinonime sau alte cuvinte similare (un pic ironic, nu-i a탳a? 游뗵)\n","\n","Chiar 탳i a탳a, tot nu vom avea reprezent캒ri pentru cuvinte inventate sau nou ap캒rute 칥n limbaj.\n","\n","[fastText](https://fasttext.cc/), ap캒rut 칥n 2016, rezolv캒 aceast캒 problem캒 prin utilizarea de n-grame la nivel de caracter. Astfel, chiar dac캒 avem un cuv칙nt necunoscut, se poate 칥ncerca ob탵inerea unei reprezent캒ri prin combinarea reprezent캒rilor n-gramelor la nivel de caracter din care e format acest cuv칙nt.\n","\n","### Reprezent캒ri contextuale ale cuvintelor\n","\n","Reprezent캒rile clasice word2vec, GloVe 탳i fastText au ca dezavantaj utilizarea unei reprezent캒ri fixe ale cuvintelor 칥nv캒탵ate, indiferent de context. De exemplu, 칥n propozi탵ia\n","```\n","Pe cer e un nor, eu cer un marker color.\n","```\n","Cuv칙ntul `cer` apare de dou캒 ori, dar are 칥n탵elesuri diferite: prima dat캒 este substantiv, a doua oar캒 este verb.\n","\n","Reprezent캒rile continue clasice de cuvinte ar furniza un singur vector pentru acest cuv칙nt, de탳i semantic nu exist캒 vreo leg캒tur캒.\n","\n","Pentru a rezolva aceast캒 problem캒, 칥n 2018 au ap캒rut reprezent캒rile contextuale ale cuvintelor. Astfel, 칥n exemplul anterior, vor exista dou캒 reprezent캒ri vectoriale pentru un singur cuv칙nt pe baza contextului.\n","\n","Exemple de reprezent캒ri contextuale: BERT, ELMo, GPT-2.\n","\n","Vezi https://ai.stanford.edu/blog/contextual/ (탳i cursurile de la masterul de NLP) pentru mai multe detalii.\n","\n","Asem캒n캒tor cu fastText, 탳i aceste modele folosesc tokeniz캒ri la nivel de \"sub-cuvinte\" (un fel de n-grame la nivel de caracter) pentru a reprezenta mai u탳or cuvinte necunoscute.\n","\n","Trebuie totu탳i men탵ionat c캒 탳i 칥n acest caz este posibil s캒 avem OOV sub-words, 칥ns캒 pentru texte pe subiecte generale este mult mai rar. Probleme de OOV 칥n prezent pot ap캒rea dac캒 folosim modele antrenate pe alte limbi, din alte domenii sau cu domenii specializate cu mul탵i termeni de ni탳캒 (de exemplu 칥n domeniul biologic sau medical).\n","\n","Cam acesta este stadiul 칥n care ne afl캒m 칥n prezent (2024) referitor la reprezent캒rile cuvintelor.\n","\n","### Alte direc탵ii\n","\n","칉n alte contexte, este posibil s캒 nu avem nevoie de granularitate la fel de mare 칥n reprezentarea textelor. Astfel, exist캒 탳i reprezent캒ri la nivel de propozi탵ie (sent2vec) sau document (doc2vec). Aceste reprezent캒ri au fost adaptate 탳i 칥n contexte mai noi (de exemplu [sentence transformers](https://www.sbert.net/)). Mai multe detalii la masterul de NLP 游뗵\n"],"metadata":{"id":"-Yi1I-KauNm-"}},{"cell_type":"markdown","metadata":{"id":"688u3vTnBmbe"},"source":["# Word2vec\n","\n","Word2vec a fost una dintre cele mai cunoscute tehnici de reprezentare a cuvintelor 칥nainte de Transformer 칥n 2017 ([arxiv](https://arxiv.org/pdf/1706.03762.pdf), [NeurIPS](https://dl.acm.org/doi/pdf/10.5555/3295222.3295349))/BERT 칥n 2018 ([arxiv](https://arxiv.org/pdf/1810.04805.pdf), [NAACL](https://aclanthology.org/N19-1423.pdf)). Word2vec a ap캒rut 칥n 2013 ([1](https://arxiv.org/pdf/1310.4546.pdf), [2](https://arxiv.org/pdf/1310.4546.pdf), [3](https://proceedings.neurips.cc/paper/2013/file/9aa42b31882ec039965f3c4923ce901b-Paper.pdf)), ideea fiind s캒 foloseasc캒 o re탵ea neural캒 cu un singur strat ascuns antrenat캒 pe fiecare cuv칙nt 칥n mod independent av칙nd ca obiectiv s캒 \"apropie\" cuvintele similare 칥n spa탵iul vectorial 탳i s캒 \"칥ndep캒rteze\" cuvintele irelevante.\n","\n","<!-- Word2vec was one the most popular embedding technique used before the rise of the Transformer in [2017](https://arxiv.org/pdf/1706.03762.pdf). It was originaly published in 2013 ([\\[1\\]](https://arxiv.org/pdf/1310.4546.pdf), [\\[2\\]](https://arxiv.org/pdf/1310.4546.pdf)) and it consists of a shallow neural network (with only one hidden layer) trained on each word from a text independently such that similar words are closer to eachother in the vector space (and unrelated words are further). -->\n","\n","Ideea de baz캒 are origini mult mai vechi 칥n filozofie:\n","- _You shall know a word by the company it keeps_ (\"Ar trebui s캒 칥n탵elegi un cuv칙nt pe baza vecinilor s캒i\" - John Rupert Firth, 1957, A synopsis of linguistic theory)\n","- _The meaning of a word is its use in the language_ (\"Semnifica탵ia unui cuv칙nt este folosirea lui 칥n limbaj\" - Ludwig Wittgenstein, 1953, Philosophical Investigations).\n","\n","Ideea este c캒 ne folosim de contextul 칥n care apare un cuv칙nt pentru a calcula similarit캒탵i 칥ntre cuvintele dintr-un text. Folosim numeroase contexte pe post de set de date de antrenare, apoi punem modelul s캒 prezic캒 reprezent캒rile cuvintelor 탵int캒. Modelul word2vec poate folosi unul dintre urm캒torii algoritmi:\n","- Continuous Bag of Words (CBoW - \"sac de cuvinte continue\"): folosim un context pentru a prezice cuv칙ntul din \"mijloc\"; merge mai bine pe seturi mici de date\n","- Skip-Gram: folosim un cuv칙nt pentru a prezice contextul din jurul s캒u; merge mai bine pe caz general (pentru cuvinte rare)\n","\n","<!-- It all starts from the quote: _You shall know a word by the company it keeps_. The idea is that we can use the context of a word to compute the similarity between different words in our text, use this as a training dataset and create a prediction model the works as an embedding for the words we have in our corpus. The model can use one of the following algorithms:\n","- Continuous Bag of Words (CBoW): use the context window around a word to predict the word; better for small datasets\n","- Skip-Gram: use a target word to predict the context around it; better at generalization (for rare words) -->"]},{"cell_type":"markdown","metadata":{"id":"i2A-MW5l8VNw"},"source":["<img src= \"https://wiki.pathmind.com/images/wiki/word2vec_diagrams.png\" width=\"500\" height=\"300\">\n","\n"]},{"cell_type":"markdown","metadata":{"id":"zpgWJMSYZscX"},"source":["<!-- [A more in depth explanation with code](https://www.tensorflow.org/text/tutorials/word2vec) -->\n","[Explica탵ie detaliat캒 cu cod](https://www.tensorflow.org/text/tutorials/word2vec)"]},{"cell_type":"markdown","metadata":{"id":"beiDzdTpwdu9"},"source":["### Continuous Bag-of-Words (CBoW) / Sac de cuvinte continue\n","\n","Spre deosebire de modelul clasic BoW, CBoW ia 칥n calcul contextul din jurul unui cuv칙nt fixat folosind o fereastr캒 de context (context window).\n","\n","De exemplu, dac캒 alegem textul _The fly will fly with you_ 탳i o fereastr캒 de dimensiune 1, algoritmul se va uita la un cuv칙nt 칥nainte 탳i un cuv칙nt dup캒, gener칙nd urm캒toarea secven탵캒 de perechi (_context_, _cuv칙nt 탵int캒_):\n","\n","<!-- Unlike the BoW model, CBoW takes into account the context around a certain word by using a context window. -->\n","\n","\n","<!-- For example, if you choose the text _The fly will fly with you._ and the window size 1, it will look at exactly 1 word before and after each word in the text, generating the following sequence of (_context_, _target_) pairs: -->\n","\n","$$([the, will], fly), ([fly, fly], will), ([will, with], fly), ([fly, you], with)$$\n","\n","Acestea sunt informa탵iile folosite de model la antrenare pentru a prezice cel mai probabil cuv칙nt d칙ndu-se un anumit context.\n","\n","<!-- This is the information on which we will train our model to predict the most probable word in a given context. -->"]},{"cell_type":"markdown","metadata":{"id":"Q-GATH4E_TxQ"},"source":["### Skip-Gram\n","\n","Modelul Skip-Gram func탵ioneaz캒 pe dos dec칙t CBoW: dat fiind un cuv칙nt 탵int캒, s캒 se prezic캒 cel mai probabil context. Pentru a realiza acest lucru, antren캒m o re탵ea cu un strat ascuns s캒 prezic캒 probabilitatea ca un cuv칙nt _y_ s캒 apar캒 l칙ng캒 un cuv칙nt _x_ 칥ntr-un text la 칥nt칙mplare. Stratul ascuns este folosit ca reprezentarea vectorial캒 a unui cuv칙nt 탵int캒. Distan탵a 칥n spa탵iul vectorial dintre 2 cuvinte ar fi mai mic캒 dac캒 acele cuvinte apar 칥n contexte similare.\n","\n","<!-- The Skip-Gram Model works the other way around: given a target word, it aims to predict the context around it. In order to do this, you can train a neural network with one hidden layer for a simple task: to predict the chance of having word _y_ really close to word _x_ in a random text. Then you use this layer as the vector representation of the given word, thus making sure that the vector distance between any 2 words is closer if they are more similar and larger if they are not. -->"]},{"cell_type":"markdown","metadata":{"id":"hPpWQth08oc-"},"source":["### Antrenarea unui model\n","<!-- ### Training a model -->"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"A3t7lt-M8q3Z"},"outputs":[],"source":["from gensim.test.utils import common_texts\n","from gensim.models import Word2Vec\n","\n","embedding = Word2Vec(\n","    sentences=common_texts,   # the list of sentences, where each sentence is given as a list of words (processed or not processed)\n","    vector_size=100,          # the number of features in the vectorized representation\n","    window=7,                 # the context window\n","    min_count=3,              # the minimum number of times a word should appear in our dataset in order to be counted\n","    sg=1                      # sg=1 means skip-gram is used, sg=0 means CBOW is used\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":278,"status":"ok","timestamp":1698665514845,"user":{"displayName":"MIRUNA-ANDREEA ZAVELCA","userId":"12692883064275182332"},"user_tz":-120},"id":"v-fzorLDPWjg","outputId":"96211085-b230-48db-88ad-530759fa4ed7"},"outputs":[{"data":{"text/plain":["{'system': 0, 'graph': 1, 'trees': 2, 'user': 3}"]},"execution_count":42,"metadata":{},"output_type":"execute_result"}],"source":["embedding.wv.key_to_index"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":224},"executionInfo":{"elapsed":8,"status":"ok","timestamp":1698665515228,"user":{"displayName":"MIRUNA-ANDREEA ZAVELCA","userId":"12692883064275182332"},"user_tz":-120},"id":"Kl1WqhlUPfLC","outputId":"1cc51037-968c-46ad-d8fa-4f327af5fb86"},"outputs":[{"data":{"text/html":["\n","  <div id=\"df-89338bf3-44f0-4f02-a44e-4367b790eb1e\" class=\"colab-df-container\">\n","    <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>0</th>\n","      <th>1</th>\n","      <th>2</th>\n","      <th>3</th>\n","      <th>4</th>\n","      <th>5</th>\n","      <th>6</th>\n","      <th>7</th>\n","      <th>8</th>\n","      <th>9</th>\n","      <th>...</th>\n","      <th>90</th>\n","      <th>91</th>\n","      <th>92</th>\n","      <th>93</th>\n","      <th>94</th>\n","      <th>95</th>\n","      <th>96</th>\n","      <th>97</th>\n","      <th>98</th>\n","      <th>99</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>system</th>\n","      <td>-0.000536</td>\n","      <td>0.000236</td>\n","      <td>0.005103</td>\n","      <td>0.009009</td>\n","      <td>-0.009303</td>\n","      <td>-0.007117</td>\n","      <td>0.006459</td>\n","      <td>0.008973</td>\n","      <td>-0.005015</td>\n","      <td>-0.003763</td>\n","      <td>...</td>\n","      <td>0.001631</td>\n","      <td>0.000190</td>\n","      <td>0.003474</td>\n","      <td>0.000218</td>\n","      <td>0.009619</td>\n","      <td>0.005061</td>\n","      <td>-0.008917</td>\n","      <td>-0.007042</td>\n","      <td>0.000901</td>\n","      <td>0.006393</td>\n","    </tr>\n","    <tr>\n","      <th>graph</th>\n","      <td>-0.008620</td>\n","      <td>0.003666</td>\n","      <td>0.005190</td>\n","      <td>0.005742</td>\n","      <td>0.007467</td>\n","      <td>-0.006168</td>\n","      <td>0.001106</td>\n","      <td>0.006047</td>\n","      <td>-0.002840</td>\n","      <td>-0.006174</td>\n","      <td>...</td>\n","      <td>0.001088</td>\n","      <td>-0.001576</td>\n","      <td>0.002197</td>\n","      <td>-0.007882</td>\n","      <td>-0.002717</td>\n","      <td>0.002663</td>\n","      <td>0.005347</td>\n","      <td>-0.002392</td>\n","      <td>-0.009510</td>\n","      <td>0.004506</td>\n","    </tr>\n","    <tr>\n","      <th>trees</th>\n","      <td>0.000095</td>\n","      <td>0.003077</td>\n","      <td>-0.006813</td>\n","      <td>-0.001375</td>\n","      <td>0.007669</td>\n","      <td>0.007346</td>\n","      <td>-0.003673</td>\n","      <td>0.002643</td>\n","      <td>-0.008317</td>\n","      <td>0.006205</td>\n","      <td>...</td>\n","      <td>-0.004509</td>\n","      <td>0.005702</td>\n","      <td>0.009180</td>\n","      <td>-0.004100</td>\n","      <td>0.007965</td>\n","      <td>0.005375</td>\n","      <td>0.005879</td>\n","      <td>0.000513</td>\n","      <td>0.008213</td>\n","      <td>-0.007019</td>\n","    </tr>\n","    <tr>\n","      <th>user</th>\n","      <td>-0.008243</td>\n","      <td>0.009299</td>\n","      <td>-0.000198</td>\n","      <td>-0.001967</td>\n","      <td>0.004604</td>\n","      <td>-0.004095</td>\n","      <td>0.002743</td>\n","      <td>0.006940</td>\n","      <td>0.006065</td>\n","      <td>-0.007511</td>\n","      <td>...</td>\n","      <td>-0.007426</td>\n","      <td>-0.001064</td>\n","      <td>-0.000795</td>\n","      <td>-0.002563</td>\n","      <td>0.009683</td>\n","      <td>-0.000459</td>\n","      <td>0.005874</td>\n","      <td>-0.007448</td>\n","      <td>-0.002506</td>\n","      <td>-0.005550</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>4 rows 칑 100 columns</p>\n","</div>\n","    <div class=\"colab-df-buttons\">\n","\n","  <div class=\"colab-df-container\">\n","    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-89338bf3-44f0-4f02-a44e-4367b790eb1e')\"\n","            title=\"Convert this dataframe to an interactive table.\"\n","            style=\"display:none;\">\n","\n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n","    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n","  </svg>\n","    </button>\n","\n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    .colab-df-buttons div {\n","      margin-bottom: 4px;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","    <script>\n","      const buttonEl =\n","        document.querySelector('#df-89338bf3-44f0-4f02-a44e-4367b790eb1e button.colab-df-convert');\n","      buttonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","      async function convertToInteractive(key) {\n","        const element = document.querySelector('#df-89338bf3-44f0-4f02-a44e-4367b790eb1e');\n","        const dataTable =\n","          await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                    [key], {});\n","        if (!dataTable) return;\n","\n","        const docLinkHtml = 'Like what you see? Visit the ' +\n","          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","          + ' to learn more about interactive tables.';\n","        element.innerHTML = '';\n","        dataTable['output_type'] = 'display_data';\n","        await google.colab.output.renderOutput(dataTable, element);\n","        const docLink = document.createElement('div');\n","        docLink.innerHTML = docLinkHtml;\n","        element.appendChild(docLink);\n","      }\n","    </script>\n","  </div>\n","\n","\n","<div id=\"df-15324094-164e-4cf5-b645-4b24705944fd\">\n","  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-15324094-164e-4cf5-b645-4b24705944fd')\"\n","            title=\"Suggest charts.\"\n","            style=\"display:none;\">\n","\n","<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","     width=\"24px\">\n","    <g>\n","        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n","    </g>\n","</svg>\n","  </button>\n","\n","<style>\n","  .colab-df-quickchart {\n","      --bg-color: #E8F0FE;\n","      --fill-color: #1967D2;\n","      --hover-bg-color: #E2EBFA;\n","      --hover-fill-color: #174EA6;\n","      --disabled-fill-color: #AAA;\n","      --disabled-bg-color: #DDD;\n","  }\n","\n","  [theme=dark] .colab-df-quickchart {\n","      --bg-color: #3B4455;\n","      --fill-color: #D2E3FC;\n","      --hover-bg-color: #434B5C;\n","      --hover-fill-color: #FFFFFF;\n","      --disabled-bg-color: #3B4455;\n","      --disabled-fill-color: #666;\n","  }\n","\n","  .colab-df-quickchart {\n","    background-color: var(--bg-color);\n","    border: none;\n","    border-radius: 50%;\n","    cursor: pointer;\n","    display: none;\n","    fill: var(--fill-color);\n","    height: 32px;\n","    padding: 0;\n","    width: 32px;\n","  }\n","\n","  .colab-df-quickchart:hover {\n","    background-color: var(--hover-bg-color);\n","    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n","    fill: var(--button-hover-fill-color);\n","  }\n","\n","  .colab-df-quickchart-complete:disabled,\n","  .colab-df-quickchart-complete:disabled:hover {\n","    background-color: var(--disabled-bg-color);\n","    fill: var(--disabled-fill-color);\n","    box-shadow: none;\n","  }\n","\n","  .colab-df-spinner {\n","    border: 2px solid var(--fill-color);\n","    border-color: transparent;\n","    border-bottom-color: var(--fill-color);\n","    animation:\n","      spin 1s steps(1) infinite;\n","  }\n","\n","  @keyframes spin {\n","    0% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","      border-left-color: var(--fill-color);\n","    }\n","    20% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    30% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","      border-right-color: var(--fill-color);\n","    }\n","    40% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    60% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","    }\n","    80% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-bottom-color: var(--fill-color);\n","    }\n","    90% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","    }\n","  }\n","</style>\n","\n","  <script>\n","    async function quickchart(key) {\n","      const quickchartButtonEl =\n","        document.querySelector('#' + key + ' button');\n","      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n","      quickchartButtonEl.classList.add('colab-df-spinner');\n","      try {\n","        const charts = await google.colab.kernel.invokeFunction(\n","            'suggestCharts', [key], {});\n","      } catch (error) {\n","        console.error('Error during call to suggestCharts:', error);\n","      }\n","      quickchartButtonEl.classList.remove('colab-df-spinner');\n","      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n","    }\n","    (() => {\n","      let quickchartButtonEl =\n","        document.querySelector('#df-15324094-164e-4cf5-b645-4b24705944fd button');\n","      quickchartButtonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","    })();\n","  </script>\n","</div>\n","    </div>\n","  </div>\n"],"text/plain":["              0         1         2         3         4         5         6   \\\n","system -0.000536  0.000236  0.005103  0.009009 -0.009303 -0.007117  0.006459   \n","graph  -0.008620  0.003666  0.005190  0.005742  0.007467 -0.006168  0.001106   \n","trees   0.000095  0.003077 -0.006813 -0.001375  0.007669  0.007346 -0.003673   \n","user   -0.008243  0.009299 -0.000198 -0.001967  0.004604 -0.004095  0.002743   \n","\n","              7         8         9   ...        90        91        92  \\\n","system  0.008973 -0.005015 -0.003763  ...  0.001631  0.000190  0.003474   \n","graph   0.006047 -0.002840 -0.006174  ...  0.001088 -0.001576  0.002197   \n","trees   0.002643 -0.008317  0.006205  ... -0.004509  0.005702  0.009180   \n","user    0.006940  0.006065 -0.007511  ... -0.007426 -0.001064 -0.000795   \n","\n","              93        94        95        96        97        98        99  \n","system  0.000218  0.009619  0.005061 -0.008917 -0.007042  0.000901  0.006393  \n","graph  -0.007882 -0.002717  0.002663  0.005347 -0.002392 -0.009510  0.004506  \n","trees  -0.004100  0.007965  0.005375  0.005879  0.000513  0.008213 -0.007019  \n","user   -0.002563  0.009683 -0.000459  0.005874 -0.007448 -0.002506 -0.005550  \n","\n","[4 rows x 100 columns]"]},"execution_count":43,"metadata":{},"output_type":"execute_result"}],"source":["import pandas as pd\n","\n","df = pd.DataFrame(\n","    [embedding.wv.get_vector(word) for word in embedding.wv.key_to_index.keys()],\n","    index=embedding.wv.key_to_index\n","  )\n","\n","df.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":12,"status":"ok","timestamp":1698665704830,"user":{"displayName":"MIRUNA-ANDREEA ZAVELCA","userId":"12692883064275182332"},"user_tz":-120},"id":"EzOAPvYRRBh3","outputId":"e719dac7-cf2c-40c1-cb86-e5bdefdc6e84"},"outputs":[{"data":{"text/plain":["[('graph', -0.01083916611969471),\n"," ('trees', -0.05234673246741295),\n"," ('user', -0.111670583486557)]"]},"execution_count":46,"metadata":{},"output_type":"execute_result"}],"source":["embedding.wv.most_similar('system')"]},{"cell_type":"markdown","metadata":{"id":"TDvBrIhFOC4c"},"source":["### 칉nc캒rcarea unui model preantrenat\n","<!-- ### Loading a pretrained model -->\n","\n","[Informa탵ii despre date 탳i modele](https://github.com/piskvorky/gensim-data)\n","<!-- [Info about data and models](https://github.com/piskvorky/gensim-data) -->\n","\n","[Exemple de utilizare](https://radimrehurek.com/gensim/models/word2vec.html)\n","<!-- [Examples on how to use](https://radimrehurek.com/gensim/models/word2vec.html) -->"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"UgUfTBUF9jhA"},"outputs":[],"source":["import gensim.downloader as api\n","\n","api.info()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/"},"id":"y7RvuW6GWujS","outputId":"dec83c65-c1d9-467c-c884-ee1e20832214"},"outputs":[{"name":"stdout","output_type":"stream","text":["[=================---------------------------------] 35.9% 597.3/1662.8MB downloaded"]}],"source":["model = api.load(\"word2vec-google-news-300\")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":7054,"status":"ok","timestamp":1698666502577,"user":{"displayName":"MIRUNA-ANDREEA ZAVELCA","userId":"12692883064275182332"},"user_tz":-120},"id":"D2TYOua6XTCm","outputId":"d12e4c50-71e2-4f24-8f7b-67d3add58020"},"outputs":[{"data":{"text/plain":["[('systems', 0.7227916717529297),\n"," ('sytem', 0.7129376530647278),\n"," ('sys_tem', 0.5871982574462891),\n"," ('System', 0.5275423526763916),\n"," ('mechanism', 0.5058810114860535),\n"," ('sysem', 0.5027822852134705),\n"," ('systen', 0.49969804286956787),\n"," ('system.The', 0.49599188566207886),\n"," ('sytems', 0.4949610233306885),\n"," ('computerized', 0.47604817152023315)]"]},"execution_count":55,"metadata":{},"output_type":"execute_result"}],"source":["model.most_similar('system')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":31,"status":"ok","timestamp":1698666502578,"user":{"displayName":"MIRUNA-ANDREEA ZAVELCA","userId":"12692883064275182332"},"user_tz":-120},"id":"wARMjmwkXYKL","outputId":"7d009206-3ddd-4556-8439-78b3a6c8f192"},"outputs":[{"data":{"text/plain":["0.09396098"]},"execution_count":56,"metadata":{},"output_type":"execute_result"}],"source":["model.similarity('system', 'graph')"]},{"cell_type":"markdown","metadata":{"id":"JxWxZU_qX19W"},"source":["### Fine-tuning (finisare) pentru modelul anterior:\n","<!-- ### Fine-tuning our model: -->\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"G7SFjytpX46K"},"outputs":[],"source":["model.train(common_texts, total_examples=4, epochs=1)"]},{"cell_type":"markdown","metadata":{"id":"xNpakyqBa_oK"},"source":["Other cool stuff:"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":636,"status":"ok","timestamp":1698666708969,"user":{"displayName":"MIRUNA-ANDREEA ZAVELCA","userId":"12692883064275182332"},"user_tz":-120},"id":"JaJ6FtC0bB1H","outputId":"e19d9f2e-85b2-429b-b05a-a22712159dd3"},"outputs":[{"data":{"text/plain":["[('queen', 0.7118193507194519),\n"," ('monarch', 0.6189674139022827),\n"," ('princess', 0.5902431011199951),\n"," ('crown_prince', 0.5499460697174072),\n"," ('prince', 0.5377321839332581),\n"," ('kings', 0.5236844420433044),\n"," ('Queen_Consort', 0.5235945582389832),\n"," ('queens', 0.5181134343147278),\n"," ('sultan', 0.5098593831062317),\n"," ('monarchy', 0.5087411999702454)]"]},"execution_count":61,"metadata":{},"output_type":"execute_result"}],"source":["model.most_similar(positive=[\"king\", \"woman\"], negative=[\"man\"])"]},{"cell_type":"markdown","metadata":{"id":"p1dZJXZ3bLwb"},"source":["[And less cool stuff:](https://arxiv.org/pdf/1607.06520.pdf)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":670,"status":"ok","timestamp":1698666714419,"user":{"displayName":"MIRUNA-ANDREEA ZAVELCA","userId":"12692883064275182332"},"user_tz":-120},"id":"_iPhuPiVbNUj","outputId":"26be2301-0515-413d-cc4f-2a2ea721462b"},"outputs":[{"data":{"text/plain":["[('homemaker', 0.5627118945121765),\n"," ('housewife', 0.5105047225952148),\n"," ('graphic_designer', 0.505180299282074),\n"," ('schoolteacher', 0.497949481010437),\n"," ('businesswoman', 0.493489146232605),\n"," ('paralegal', 0.49255111813545227),\n"," ('registered_nurse', 0.4907974898815155),\n"," ('saleswoman', 0.4881627559661865),\n"," ('electrical_engineer', 0.4797725975513458),\n"," ('mechanical_engineer', 0.4755399227142334)]"]},"execution_count":62,"metadata":{},"output_type":"execute_result"}],"source":["model.most_similar(positive=[\"computer_programmer\", \"woman\"], negative=[\"man\"])"]},{"cell_type":"markdown","metadata":{"id":"H_dqRxhjbnT_"},"source":["Bias is still an unsolved problem in Machine Learning. Do you know any other popular examples of bias?"]},{"cell_type":"markdown","metadata":{"id":"55UuO7SZRcJs"},"source":["# Principal Component Analysis (PCA) / Analiza componentelor principale\n","\n","PCA este un algoritm de reducere a num캒rului de dimensiuni -- poate fi util pentru a vizualiza date cu sute de dimensiuni 칥ntr-un spa탵iu 2D sau 3D. De exemplu, pentru a observa distan탵a dintre proiec탵iile unor cuvinte, avem:\n","\n","<!-- PCA is a dimensionality reduction algorithm -- meaning that we can use it to visualise our data in 2D or 3D. Here is an example of how you can use it to see the distance between embeddings in 2D: -->"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"iSYbiUpQMaM-"},"outputs":[],"source":["from sklearn.decomposition import PCA\n","\n","text = ['system', 'graph', 'trees', 'user']\n","embeddings = [model[word] for word in text]\n","\n","pca = PCA(n_components=2)\n","pca.fit(embeddings)\n","vectors_2d = pca.transform(embeddings)"]},{"cell_type":"markdown","source":["Interfa탵a din sklearn este asem캒n캒toare ca cea a unui model de ML. Dup캒 ce antren캒m modelul, putem pune componentele pe un grafic cu matplotlib:\n","\n","<!-- We can train it the same way we would a normal ML model, and visualize the results using, for example, a plotting library like matplotlib: -->"],"metadata":{"id":"K3OgyZ5CB4Wh"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"oxhT5BnPQZXI"},"outputs":[],"source":["import matplotlib.pyplot as plt\n","\n","x = [v[0] for v in vectors_2d]\n","y = [v[1] for v in vectors_2d]\n","\n","fig, ax = plt.subplots()\n","ax.scatter(x, y)\n","\n","for i, txt in enumerate(text):\n","    ax.annotate(txt, (x[i], y[i]))\n","\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"TiHaJDLOgYnp"},"source":["## Global Vectors (GloVe) / Vectori globali\n","\n","Word2Vec se bazeaz캒 pe statistici locale (apari탵ii la nivel de propozi탵ie). [GloVe](https://nlp.stanford.edu/projects/glove/) ia 칥n calcul statistici globale, ceea ce poate fi util pentru seturi de date mici, nefiind nevoie de multe date de antrenare.\n","\n","Modelul num캒r캒 toate perechile \"cuv칙nt1 cuv칙nt2 ...\" (pentru un context de dimensiune x consider캒m cuvinte cu distan탵a cel mult x 칥ntre ele) 탳i re탵ine informa탵ia 칥ntr-o matrice de num캒r de apari탵ii (co-occurrence matrix):\n","\n","<!-- While Word2Vec is based only on local statistics (the occurence of words at\n","a single-sentence level) [GloVe](https://nlp.stanford.edu/projects/glove/) incorporates global statistics methods. This makes it better suited for smaller datasets, as it does not need as much training data. -->\n","\n","<!-- The model counts all \"word1 word2 ...\" pairs (for a context window of x we consider words that have at most distance x between them) and keeps the information in a co-occurrence matrix: -->"]},{"cell_type":"markdown","metadata":{"id":"dJ7WU2agz4b2"},"source":["<center><img src='https://drive.google.com/uc?export=view&id=1pnX1lPdQItUauHp9W8xJlx8q2lgTe4cJ' width=500></center>"]},{"cell_type":"markdown","metadata":{"id":"V6Y4Noij2Egg"},"source":["Dup캒 aceea, se calculeaz캒 probabilitatea ca un cuv칙nt s캒 fie mai aproape de alt cuv칙nt pe baza acestei matrice:\n","\n","<!-- Afterwards, it computes the probability that a word will be closer to another one based on this matrix: -->\n","$$P(j | i) = \\frac{X_{ij}}{X_i}$$\n","<!-- where: -->\n","unde:\n","$$P(j | i) = \\text{probabilitatea s캒 avem cuv칙ntul j dac캒 avem cuv칙ntul i}$$\n","$$X_{ij} = \\text{de c칙te ori apare cuv칙ntul j 칥n contextul cuv칙ntului i}$$\n","$$X_i = \\sum_k X_{ik} = \\text{num캒rul total de cuvinte care apar 칥n contextul cuv칙ntului i}$$\n","\n","<!-- $$P(j | i) = the\\ probability\\ of\\ word\\ j\\ given\\ i$$\n","$$X_{ij} = how\\ many\\ times\\ word\\ j\\ appears\\ in\\ the\\ context\\ of\\ i$$\n","$$X_i = \\sum_k X_{ik} = sum\\ of\\ how\\ many\\ times\\ words\\ appear\\ in\\ the\\ context\\ of\\ i$$ -->"]},{"cell_type":"markdown","metadata":{"id":"qg2yYtcC_TIJ"},"source":["Pe baza acestor calcule, ar trebui s캒 putem determina rela탵ii 칥ntre cuvinte:\n","\n","<!-- Based on this we should be able to infer relations between words: -->\n","\n","<center><img src='https://nlp.stanford.edu/projects/glove/images/table.png' width=500></center>\n","\n","Observ캒m c캒 _solid_ are leg캒tur캒 cu _ice_, dar nu 탳i cu _steam_, pe c칙nd _gas_ are leg캒tur캒 cu _steam_, dar nu 탳i cu _ice_ (probabilit캒탵i condi탵ionale foarte mari vs foarte mici). _Water_ 탳i _fasion_ sunt fie str칙ns corelate cu _ice_ 탳i _steam_ 칥mpreun캒, fie sunt complet f캒r캒 leg캒tur캒.\n","\n","<!-- Notice how _solid_ is related to _ice_ but not _steam_, while _gas_ is related to _steam_ but not _ice_ (very large vs. very small conditional values). _Water_ and _fashion_ on the other hand are either highly related to both or completely unrelated. -->"]},{"cell_type":"markdown","metadata":{"id":"oW_-byU0Ad53"},"source":["Mai multe detalii se reg캒sesc 칥n [articol](https://aclanthology.org/D14-1162.pdf).\n","\n","<!-- Some more computation will bring us to the regression model that is now used for this model. If you want to learn more you can check [the paper](https://aclanthology.org/D14-1162.pdf). -->"]},{"cell_type":"markdown","metadata":{"id":"cFdqo-FRQ7gP"},"source":["### Utilizarea GloVe\n","<!-- ### Using GloVe -->\n","\n","Putem 칥nc캒rca un model GloVe preantrenat folosind biblioteca gensim (sau alte resurse):\n","<!-- We can load a pretrained GloVe model using the gensim library (or other resources): -->"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":243152,"status":"ok","timestamp":1698748384714,"user":{"displayName":"MIRUNA-ANDREEA ZAVELCA","userId":"12692883064275182332"},"user_tz":-120},"id":"0mY_dP3FQ6_n","outputId":"7db963e5-d07a-426a-a131-2ffbfdc9e46d"},"outputs":[{"name":"stdout","output_type":"stream","text":["[==================================================] 100.0% 387.1/387.1MB downloaded\n"]}],"source":["import gensim.downloader as api\n","\n","model = api.load(\"glove-twitter-100\")"]},{"cell_type":"markdown","metadata":{"id":"jUKW1ADEVX5d"},"source":["Pentru a calcula reprezent캒rile cuvintelor (word embeddings) sau pentru alte similarit캒탵i 칥ntre cuvinte, la fel ca 칥n cazul Word2Vec:\n","<!-- And use it to compute the word embeddings (or do all other similarity functions that we saw for Word2Vec): -->"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":289,"status":"ok","timestamp":1698749051277,"user":{"displayName":"MIRUNA-ANDREEA ZAVELCA","userId":"12692883064275182332"},"user_tz":-120},"id":"P6ZEmJuKR9S9","outputId":"e937c2a9-78d0-46aa-ea3a-245a9f827a21"},"outputs":[{"data":{"text/plain":["array([ 0.43887 ,  0.32601 , -0.28524 , -0.08248 ,  0.43643 ,  0.75065 ,\n","        0.093945, -0.72626 ,  0.32297 , -0.37128 , -0.23306 ,  0.35499 ,\n","       -3.1764  ,  0.015004,  0.69725 , -0.15256 ,  0.025449, -0.058944,\n","        0.20002 , -0.61298 , -0.79661 ,  0.53051 ,  0.64765 ,  0.90153 ,\n","       -0.27407 ,  0.52871 ,  0.39344 ,  0.56076 ,  0.31942 ,  0.83347 ,\n","       -0.53268 , -1.0166  , -0.25328 , -0.17347 ,  0.68794 ,  0.25902 ,\n","        0.42864 ,  0.3844  , -0.071415, -0.026013, -0.42733 ,  0.58874 ,\n","       -0.30061 , -0.18357 ,  0.21158 , -0.72648 , -0.48477 ,  0.43527 ,\n","       -0.37412 , -0.48493 ,  0.26264 ,  0.21684 , -0.8822  ,  0.57925 ,\n","       -0.54    ,  0.7147  , -0.33133 , -0.44715 , -0.40713 , -0.014364,\n","       -0.083808,  0.45569 , -0.094374,  0.56057 ,  0.65446 , -0.45768 ,\n","        0.2522  ,  0.34328 , -0.061001, -0.4899  ,  0.3342  ,  0.41277 ,\n","       -0.55403 ,  0.30807 ,  0.22867 , -0.53921 ,  0.16439 ,  0.021561,\n","        0.15131 , -0.70287 ,  1.4152  ,  0.83387 ,  0.44385 , -0.042976,\n","        0.069162, -0.74432 , -0.032278, -0.6221  ,  0.20007 ,  0.15834 ,\n","       -0.53907 , -0.31442 ,  0.60969 , -0.32378 ,  0.1676  , -0.94943 ,\n","        0.52916 ,  0.035842, -0.041395, -0.56533 ], dtype=float32)"]},"execution_count":3,"metadata":{},"output_type":"execute_result"}],"source":["model['system']"]},{"cell_type":"markdown","metadata":{"id":"Sc22faRVaCpg"},"source":["Pentru a antrena un model de la zero:\n","<!-- Or you can train your own model from scratch: -->"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kG2UypDfaTjG"},"outputs":[],"source":["from glove import Corpus, Glove\n","\n","corpus = Corpus()\n","corpus.fit(common_texts, window=4)\n","\n","glove = Glove(no_components=4, learning_rate=0.1)\n","glove.fit(corpus.matrix, epochs=10, no_threads=8, verbose=True)\n","glove.add_dictionary(corpus.dictionary)\n","glove.save('glove.model.txt')"]},{"cell_type":"markdown","metadata":{"id":"uidJv81en9Qh"},"source":["# Principal Component Analysis (PCA)\n","\n","PCA is a dimensionality reduction algorithm -- meaning that we can use it to visualise our data in 2D or 3D. Here is an example of how you can use it to see the distance between embeddings in 2D:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_qkjsUVBn9Qi"},"outputs":[],"source":["from sklearn.decomposition import PCA\n","\n","text = ['system', 'graph', 'trees', 'user']\n","embeddings = [model[word] for word in text]\n","\n","pca = PCA(n_components=2)\n","pca.fit(embeddings)\n","vectors_2d = pca.transform(embeddings)"]},{"cell_type":"markdown","source":["We can train it the same way we would a normal ML model, and visualize the results using, for example, a plotting library like matplotlib:"],"metadata":{"id":"p-ftSWuZn9Qi"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"l4Gg4VGSn9Qi"},"outputs":[],"source":["import matplotlib.pyplot as plt\n","\n","x = [v[0] for v in vectors_2d]\n","y = [v[1] for v in vectors_2d]\n","\n","fig, ax = plt.subplots()\n","ax.scatter(x, y)\n","\n","for i, txt in enumerate(text):\n","    ax.annotate(txt, (x[i], y[i]))\n","\n","plt.show()"]},{"cell_type":"markdown","source":["## Exerci탵ii\n","\n","1. Scrie propria implementare de Bag of Words de la zero. Ar trebui s캒 furnizeze at칙t reprezent캒ri binare, c칙t 탳i reprezent캒ri pentru num캒rul de apari탵ii.\n","2. Implementeaz캒 Tf-Idf de la zero. Define탳te oric칙te func탵ii ajut캒toare crezi necesare.\n","3. Vezi distan탵a dintre ni탳te cuvinte 칥n 2D folosind PCA (sau alt캒 modalitate de reducere a dimensionalit캒탵ii).\n","4. Crea탵i perechi de (context, cuv칙nt_탵int캒) 탳i antrena탵i o re탵ea neural캒 folosind skip-gram sau continuous bag of words. Ar trebui s캒 eticheta탵i fiecare cuv칙nt cu un ID unic 탳i s캒 folosi탵i padding pentru antrenarea cuvintelor din capete (context \"dummy\" 칥n st칙nga/dreapta).\n","5. Vizualiza탵i distan탵ele dintre c칙teva cuvinte 칥n 2D folosind PCA (sau alte tehnici de reducere a num캒rului de dimensiuni).\n","6. Compara탵i reprezent캒rile de cuvinte 칥n diverse moduri, de exemplu timp de antrenare, cel mai similar cuv칙nt pentru cuv칙ntul X, distan탵e 칥n spa탵iul 2D, acurate탵ea cu SVM etc. Compara탵i implement캒rile voastre cu implement캒rile furnizate de biblioteci."],"metadata":{"id":"gAkdr9isbbmk"}}],"metadata":{"colab":{"collapsed_sections":["sUqzZuWWhUuw","yLcyp5Ed6QKT"],"provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}