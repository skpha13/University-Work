{"cells":[{"cell_type":"markdown","metadata":{"id":"jEuuzNUQ_MSG"},"source":["<!-- # Word Representations -->\n","# Reprezentarea Cuvintelor\n","\n","În procesarea limbajului natural, word embeddings (reprezentarea cuvintelor prin proiecții) este termenul folosit pentru reprezentarea cuvintelor ca vectori. Pentru a antrena un model, avem nevoie de date numerice, deci avem nevoie de o modalitate prin care să transformăm un text în vectori de numere încercând să păstrăm cât mai multe informații relevante pentru ce vrem să facem. Așadar, uneori ne interesează relațiile semantice, alteori conținutul lexical șamd.\n","\n","<!-- In natural language processing, word embedding is the term used for representing a word as a vector. For training a model we need numerical data, which means that we must find a way to represent texts such that we keep as much information as possible considering our current context. This means that sometimes semantic relations will be more important, other times lexical information etc. -->\n","\n","Prin vectorizarea cuvintelor, reprezentăm fiecare cuvânt ca un număr sau ca o listă de numere. În cazul reprezentărilor dense/continue ale cuvintelor, ideea este să reprezentăm cuvinte similare ca fiind apropiate în spațiul vectorial în care le proiectăm.\n","\n","<!-- By using word embeddings (vectorization) we can represent each word as a number or a list of numbers that conveys this information such that words that are similar will be closer to each other in the vector space than words that are not. -->\n","\n","[word2vec ilustrat](https://jalammar.github.io/illustrated-word2vec/) (revenim la word2vec ora viitoare)"]},{"cell_type":"markdown","metadata":{"id":"sUqzZuWWhUuw"},"source":["# Bag of Words (BoW) / Sac de cuvinte\n","\n","Pentru situațiile când contextul și ordinea cuvintelor nu este relevantă, ci doar cât de des apar cuvintele, atunci am folosi BoW. E ca și cum am \"arunca\" toate cuvintele într-un sac (eventual le și amestecăm) iar apoi numărăm de câte ori apare fiecare cuvânt (un fel de vector de frecvență). Ca un caz particular, putem avea BoW binar (un fel de vector de apariții). Aceasta este cam cea mai simplă tehnică de vectorizat un text.\n","\n","<!-- Imagine a situation where the context of the words is not relevant, only how often they appear. This is where we use bag of words. This approach just throws all words in a bag, maybe shuffles it a bit, then counts how many times each words appears (or if they appear in case of a binary BoW). It is the easiest vectorization method that we will discuss. -->"]},{"cell_type":"markdown","metadata":{"id":"IS2s198nbhgT"},"source":["<center><img src='https://drive.google.com/uc?export=view&id=1v6McR199QkVXvuQmC3FWJ80rSXGTbZUS' width=500></center>"]},{"cell_type":"markdown","metadata":{"id":"3AExe5lbiUNm"},"source":["Să luăm exemplul din imagine dat ca exemplu. Fie scriem noi implementarea de la zero, fie folosim [CountVectorizer](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html) gata implementat în scikit-learn:\n","\n","<!-- Let's take the text from the example. We can either write our own BoW implementation, or we can use the one preimplemented in [CountVectorizer](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html): -->"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":264,"status":"ok","timestamp":1698067653684,"user":{"displayName":"MIRUNA-ANDREEA ZAVELCA","userId":"12692883064275182332"},"user_tz":-180},"id":"ymHBxvsQiKXV","outputId":"31f8c0fe-ec94-4286-e82c-ba2a24217bd2"},"outputs":[{"name":"stdout","output_type":"stream","text":["['did' 'fly' 'see' 'the' 'will' 'with' 'you']\n","[[1 1 1 1 0 0 1]\n"," [0 2 0 1 1 1 1]]\n"]}],"source":["from sklearn.feature_extraction.text import CountVectorizer\n","\n","text = ['Did you see the fly?', 'The fly will fly with you.']\n","vectorizer = CountVectorizer()\n","X = vectorizer.fit_transform(text)\n","print(vectorizer.get_feature_names_out())\n","print(X.toarray())"]},{"cell_type":"markdown","metadata":{"id":"RGKVdy4-sgoy"},"source":["`CountVectorizer` are un constructor cu mulți parametri impliciți pe care îi putem suprascrie. De exemplu, dacă vrem doar reprezentare binară (vector de apariții) și să numărăm doar bigrame, am proceda astfel:\n","\n","<!-- CountVectorizer is a class with predefined parameters. You can always change those parameters, meaning that you can, for example, choose to have a binary representation of bigrams: -->"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":651,"status":"ok","timestamp":1698067678802,"user":{"displayName":"MIRUNA-ANDREEA ZAVELCA","userId":"12692883064275182332"},"user_tz":-180},"id":"8n3BFJtLpb2H","outputId":"223bcc80-aac6-4250-a68c-bd2f856b2604"},"outputs":[{"name":"stdout","output_type":"stream","text":["['am not' 'he is' 'is very' 'not happy' 'very happy']\n","[[1 0 0 1 0]\n"," [0 1 1 0 1]]\n"]}],"source":["text = ['I am not happy.', 'He is very happy']\n","vectorizer = CountVectorizer(binary=True, ngram_range=(2, 2))\n","X = vectorizer.fit_transform(text)\n","print(vectorizer.get_feature_names_out())\n","print(X.toarray())"]},{"cell_type":"markdown","metadata":{"id":"ThLRQJI3uLyc"},"source":["N-gramele (la nivel de cuvânt) sunt secvențe de n cuvinte. Ele sunt utile pentru a furniza context, de exemplu pentru a diferenția între _not happy_ și _verry happy_. Le putem folosi fie ca features (reprezentări numerice), fie ca să analizăm setul de date.\n","\n","<!-- N-grams are sequences of n words. They help us get some context about the text, letting us know the difference between _not happy_ and _very happy_ for example. This can be used as a feature for another representation, or on its own to make assumptions about the dataset. -->"]},{"cell_type":"markdown","metadata":{"id":"yLcyp5Ed6QKT"},"source":["#  Term Frequency - Inverse Document Frequency (Tf-idf)\n","\n","Doar pentru că un cuvânt apare de multe ori, nu înseamnă că este și relevant dpdv al conținutului (vezi stopwords). De asemenea, stopwords variază de la un domeniu la altul și poate fi anevoios să ne definim manual de fiecare dată liste de stopwords. Dacă de exemplu vrem să ne facem un motor de căutare, ar fi mai relevant un cuvânt care apare des într-un document, însă nu este frecvent întâlnit în celelalte documente (cum se întâmplă în cazul stopwords).\n","\n","<!-- Just because a word appears often it does not mean that it is necessarily relevant (think about stopwords). If we want to write a search engine for example, it would be more relevant for us to know how often a certain word appears in a document with regards to how common that word generally is. Tf-idf is an algorithm that takes this into account. In other words, a word is important for a given document if it appears many times in this one and rarely in others. -->"]},{"cell_type":"markdown","metadata":{"id":"KaNvpl8r1teo"},"source":["Pentru un document dat, repetăm următoarele operații pentru fiecare cuvânt din întreg setul de date:\n","\n","<!-- We will consider the given document as the current datapoint and repeat the following for each word in the dataset: -->\n","\n","$$TFIDF = TF * IDF$$\n","unde:\n","<!-- $$TF(word, document) = \\frac{How\\ many\\ times\\ the\\ word\\ appears\\ in\\ the\\ document}{Number\\ of\\ words\\ in\\ the\\ document}$$ -->\n","$$TF(cuvânt, document) = \\frac{\\text{#apariții ale cuvântului în document}}{\\text{# de cuvinte din document}}$$\n","și:\n","$$IDF(cuvânt, Documente) = log(\\frac{\\text{# de documente din corpus}}{1 + \\text{# de documente care conțin cuvântul curent}} + 1)$$\n","\n","<!-- We use **log** in order to smooth our values for an easier analysis. -->\n","Folosim **log** pentru a normaliza valorile și pentru a fi mai ușor de analizat."]},{"cell_type":"markdown","metadata":{"id":"VDamrGeM6H5I"},"source":["Pentru implementare, avem [TfidfVectorizer](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html). Reprezentarea va fi o matrice unde fiecare rând corespunde unui document și fiecare coloană corespunde unui cuvânt din tot setul de date:\n","\n","<!-- For the implementation you can use [TfidfVectorizer](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html). The output will be a matrix where each row corresponds to a datapoint and each column to a word from the full dataset: -->"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":408,"status":"ok","timestamp":1698087478538,"user":{"displayName":"MIRUNA-ANDREEA ZAVELCA","userId":"12692883064275182332"},"user_tz":-180},"id":"PA5hy4bW4d78","outputId":"8155320d-476e-4dc6-d653-99a620cadc31"},"outputs":[{"name":"stdout","output_type":"stream","text":["['am' 'happy' 'he' 'is' 'not' 'very']\n","[[0.6316672  0.44943642 0.         0.         0.6316672  0.        ]\n"," [0.         0.37997836 0.53404633 0.53404633 0.         0.53404633]]\n"]}],"source":["from sklearn.feature_extraction.text import TfidfVectorizer\n","\n","text = ['I am not happy.', 'He is very happy']\n","\n","vectorizer = TfidfVectorizer()\n","X = vectorizer.fit_transform(text)\n","print(vectorizer.get_feature_names_out())\n","print(X.toarray())"]},{"cell_type":"markdown","source":["# Reprezentări continue ale cuvintelor\n","\n","Reprezentările de până acum (BoW, Tf-Idf) au ca dezavantaje\n","- necesitatea unui număr extrem de mare de elemente pe măsură ce avem texte de dimensiuni mai mari\n","- nu au un mod de a reprezenta relațiile semantice dintre cuvinte\n","\n","Pentru a ocupa mai puțină memorie, se folosesc reprezentări cu matrici rare (sparse matrix). Totuși, chiar și așa, aceste reprezentări rare ale cuvintelor nu scalează când dimensiunea vocabularului este foarte mare.\n","\n","Ca soluții la această problemă, au apărut reprezentările dense (continue) de cuvinte, unde numărul de dimensiuni ale proiecțiilor cuvintelor este mult mai mic.\n","\n","Dimensiunea unui vocabular poate ajunge la câteva zeci sau sute de mii de cuvinte (și chiar milioane), deci utilizarea unor matrice de astfel de dimensiuni în rețele neurale devine extrem de costisitoare.\n","\n","### Pe scurt despre word2vec (2013) și GloVe (2014)\n","\n","Reprezentările continue sunt pe scurt o matrice utilizată pentru a reduce dimensiunea unui vector. Dacă dimensiunea vocabularului este V și reducem la niște embeddings (reprezentări/proiecții) de dimensiune N, avem o matrice de $V*N$. Inițial, din textul nostru vom reprezenta un cuvânt ca un vector one-hot de dimensiune $V$.\n","\n","Pentru a obține reprezentarea cuvântului, înmulțim vectorul cu matricea de proiecție (embedding matrix), deci obținem un vector de dimensiune $N$.\n","\n","De exemplu, dimensiunea vocabularului este de $10^5$ și reducem la niște reprezentări de dimensiune 300. Astfel, în loc să lucrăm cu vectori de dimensiune $10^5$ în rețelele noastre, avem doar vectori de dimensiuni 300, mult mai mici.\n","\n","### fastText\n","\n","Cu toate că reprezentările word2vec și GloVE ne rezolvă problemele cu reprezentările rare ale cuvintelor, acestea au o limitare (existentă și înainte) referitoare la modul în care ar trebui tratate cuvintele care nu există în setul de date inițial.\n","\n","Cuvintele din afara vocabularului (OOVW - out of vocabulary words) nu pot fi reprezentate în spațiul vectorial dat de matricea de proiecție, deoarece nu apar în vocabular. Câteva metode de remediere:\n","- cel mai simplu, adăugăm în vocabular un cuvând UNK (unknown - necunoscut)\n","- folosim stemming sau lematizare în speranța că vom găsi un cuvânt similar în vocabularul inițial\n","- căutăm sinonime sau alte cuvinte similare (un pic ironic, nu-i așa? 🙂)\n","\n","Chiar și așa, tot nu vom avea reprezentări pentru cuvinte inventate sau nou apărute în limbaj.\n","\n","[fastText](https://fasttext.cc/), apărut în 2016, rezolvă această problemă prin utilizarea de n-grame la nivel de caracter. Astfel, chiar dacă avem un cuvânt necunoscut, se poate încerca obținerea unei reprezentări prin combinarea reprezentărilor n-gramelor la nivel de caracter din care e format acest cuvânt.\n","\n","### Reprezentări contextuale ale cuvintelor\n","\n","Reprezentările clasice word2vec, GloVe și fastText au ca dezavantaj utilizarea unei reprezentări fixe ale cuvintelor învățate, indiferent de context. De exemplu, în propoziția\n","```\n","Pe cer e un nor, eu cer un marker color.\n","```\n","Cuvântul `cer` apare de două ori, dar are înțelesuri diferite: prima dată este substantiv, a doua oară este verb.\n","\n","Reprezentările continue clasice de cuvinte ar furniza un singur vector pentru acest cuvânt, deși semantic nu există vreo legătură.\n","\n","Pentru a rezolva această problemă, în 2018 au apărut reprezentările contextuale ale cuvintelor. Astfel, în exemplul anterior, vor exista două reprezentări vectoriale pentru un singur cuvânt pe baza contextului.\n","\n","Exemple de reprezentări contextuale: BERT, ELMo, GPT-2.\n","\n","Vezi https://ai.stanford.edu/blog/contextual/ (și cursurile de la masterul de NLP) pentru mai multe detalii.\n","\n","Asemănător cu fastText, și aceste modele folosesc tokenizări la nivel de \"sub-cuvinte\" (un fel de n-grame la nivel de caracter) pentru a reprezenta mai ușor cuvinte necunoscute.\n","\n","Trebuie totuși menționat că și în acest caz este posibil să avem OOV sub-words, însă pentru texte pe subiecte generale este mult mai rar. Probleme de OOV în prezent pot apărea dacă folosim modele antrenate pe alte limbi, din alte domenii sau cu domenii specializate cu mulți termeni de nișă (de exemplu în domeniul biologic sau medical).\n","\n","Cam acesta este stadiul în care ne aflăm în prezent (2024) referitor la reprezentările cuvintelor.\n","\n","### Alte direcții\n","\n","În alte contexte, este posibil să nu avem nevoie de granularitate la fel de mare în reprezentarea textelor. Astfel, există și reprezentări la nivel de propoziție (sent2vec) sau document (doc2vec). Aceste reprezentări au fost adaptate și în contexte mai noi (de exemplu [sentence transformers](https://www.sbert.net/)). Mai multe detalii la masterul de NLP 🙂\n"],"metadata":{"id":"-Yi1I-KauNm-"}},{"cell_type":"markdown","metadata":{"id":"688u3vTnBmbe"},"source":["# Word2vec\n","\n","Word2vec a fost una dintre cele mai cunoscute tehnici de reprezentare a cuvintelor înainte de Transformer în 2017 ([arxiv](https://arxiv.org/pdf/1706.03762.pdf), [NeurIPS](https://dl.acm.org/doi/pdf/10.5555/3295222.3295349))/BERT în 2018 ([arxiv](https://arxiv.org/pdf/1810.04805.pdf), [NAACL](https://aclanthology.org/N19-1423.pdf)). Word2vec a apărut în 2013 ([1](https://arxiv.org/pdf/1310.4546.pdf), [2](https://arxiv.org/pdf/1310.4546.pdf), [3](https://proceedings.neurips.cc/paper/2013/file/9aa42b31882ec039965f3c4923ce901b-Paper.pdf)), ideea fiind să folosească o rețea neurală cu un singur strat ascuns antrenată pe fiecare cuvânt în mod independent având ca obiectiv să \"apropie\" cuvintele similare în spațiul vectorial și să \"îndepărteze\" cuvintele irelevante.\n","\n","<!-- Word2vec was one the most popular embedding technique used before the rise of the Transformer in [2017](https://arxiv.org/pdf/1706.03762.pdf). It was originaly published in 2013 ([\\[1\\]](https://arxiv.org/pdf/1310.4546.pdf), [\\[2\\]](https://arxiv.org/pdf/1310.4546.pdf)) and it consists of a shallow neural network (with only one hidden layer) trained on each word from a text independently such that similar words are closer to eachother in the vector space (and unrelated words are further). -->\n","\n","Ideea de bază are origini mult mai vechi în filozofie:\n","- _You shall know a word by the company it keeps_ (\"Ar trebui să înțelegi un cuvânt pe baza vecinilor săi\" - John Rupert Firth, 1957, A synopsis of linguistic theory)\n","- _The meaning of a word is its use in the language_ (\"Semnificația unui cuvânt este folosirea lui în limbaj\" - Ludwig Wittgenstein, 1953, Philosophical Investigations).\n","\n","Ideea este că ne folosim de contextul în care apare un cuvânt pentru a calcula similarități între cuvintele dintr-un text. Folosim numeroase contexte pe post de set de date de antrenare, apoi punem modelul să prezică reprezentările cuvintelor țintă. Modelul word2vec poate folosi unul dintre următorii algoritmi:\n","- Continuous Bag of Words (CBoW - \"sac de cuvinte continue\"): folosim un context pentru a prezice cuvântul din \"mijloc\"; merge mai bine pe seturi mici de date\n","- Skip-Gram: folosim un cuvânt pentru a prezice contextul din jurul său; merge mai bine pe caz general (pentru cuvinte rare)\n","\n","<!-- It all starts from the quote: _You shall know a word by the company it keeps_. The idea is that we can use the context of a word to compute the similarity between different words in our text, use this as a training dataset and create a prediction model the works as an embedding for the words we have in our corpus. The model can use one of the following algorithms:\n","- Continuous Bag of Words (CBoW): use the context window around a word to predict the word; better for small datasets\n","- Skip-Gram: use a target word to predict the context around it; better at generalization (for rare words) -->"]},{"cell_type":"markdown","metadata":{"id":"i2A-MW5l8VNw"},"source":["<img src= \"https://wiki.pathmind.com/images/wiki/word2vec_diagrams.png\" width=\"500\" height=\"300\">\n","\n"]},{"cell_type":"markdown","metadata":{"id":"zpgWJMSYZscX"},"source":["<!-- [A more in depth explanation with code](https://www.tensorflow.org/text/tutorials/word2vec) -->\n","[Explicație detaliată cu cod](https://www.tensorflow.org/text/tutorials/word2vec)"]},{"cell_type":"markdown","metadata":{"id":"beiDzdTpwdu9"},"source":["### Continuous Bag-of-Words (CBoW) / Sac de cuvinte continue\n","\n","Spre deosebire de modelul clasic BoW, CBoW ia în calcul contextul din jurul unui cuvânt fixat folosind o fereastră de context (context window).\n","\n","De exemplu, dacă alegem textul _The fly will fly with you_ și o fereastră de dimensiune 1, algoritmul se va uita la un cuvânt înainte și un cuvânt după, generând următoarea secvență de perechi (_context_, _cuvânt țintă_):\n","\n","<!-- Unlike the BoW model, CBoW takes into account the context around a certain word by using a context window. -->\n","\n","\n","<!-- For example, if you choose the text _The fly will fly with you._ and the window size 1, it will look at exactly 1 word before and after each word in the text, generating the following sequence of (_context_, _target_) pairs: -->\n","\n","$$([the, will], fly), ([fly, fly], will), ([will, with], fly), ([fly, you], with)$$\n","\n","Acestea sunt informațiile folosite de model la antrenare pentru a prezice cel mai probabil cuvânt dându-se un anumit context.\n","\n","<!-- This is the information on which we will train our model to predict the most probable word in a given context. -->"]},{"cell_type":"markdown","metadata":{"id":"Q-GATH4E_TxQ"},"source":["### Skip-Gram\n","\n","Modelul Skip-Gram funcționează pe dos decât CBoW: dat fiind un cuvânt țintă, să se prezică cel mai probabil context. Pentru a realiza acest lucru, antrenăm o rețea cu un strat ascuns să prezică probabilitatea ca un cuvânt _y_ să apară lângă un cuvânt _x_ într-un text la întâmplare. Stratul ascuns este folosit ca reprezentarea vectorială a unui cuvânt țintă. Distanța în spațiul vectorial dintre 2 cuvinte ar fi mai mică dacă acele cuvinte apar în contexte similare.\n","\n","<!-- The Skip-Gram Model works the other way around: given a target word, it aims to predict the context around it. In order to do this, you can train a neural network with one hidden layer for a simple task: to predict the chance of having word _y_ really close to word _x_ in a random text. Then you use this layer as the vector representation of the given word, thus making sure that the vector distance between any 2 words is closer if they are more similar and larger if they are not. -->"]},{"cell_type":"markdown","metadata":{"id":"hPpWQth08oc-"},"source":["### Antrenarea unui model\n","<!-- ### Training a model -->"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"A3t7lt-M8q3Z"},"outputs":[],"source":["from gensim.test.utils import common_texts\n","from gensim.models import Word2Vec\n","\n","embedding = Word2Vec(\n","    sentences=common_texts,   # the list of sentences, where each sentence is given as a list of words (processed or not processed)\n","    vector_size=100,          # the number of features in the vectorized representation\n","    window=7,                 # the context window\n","    min_count=3,              # the minimum number of times a word should appear in our dataset in order to be counted\n","    sg=1                      # sg=1 means skip-gram is used, sg=0 means CBOW is used\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":278,"status":"ok","timestamp":1698665514845,"user":{"displayName":"MIRUNA-ANDREEA ZAVELCA","userId":"12692883064275182332"},"user_tz":-120},"id":"v-fzorLDPWjg","outputId":"96211085-b230-48db-88ad-530759fa4ed7"},"outputs":[{"data":{"text/plain":["{'system': 0, 'graph': 1, 'trees': 2, 'user': 3}"]},"execution_count":42,"metadata":{},"output_type":"execute_result"}],"source":["embedding.wv.key_to_index"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":224},"executionInfo":{"elapsed":8,"status":"ok","timestamp":1698665515228,"user":{"displayName":"MIRUNA-ANDREEA ZAVELCA","userId":"12692883064275182332"},"user_tz":-120},"id":"Kl1WqhlUPfLC","outputId":"1cc51037-968c-46ad-d8fa-4f327af5fb86"},"outputs":[{"data":{"text/html":["\n","  <div id=\"df-89338bf3-44f0-4f02-a44e-4367b790eb1e\" class=\"colab-df-container\">\n","    <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>0</th>\n","      <th>1</th>\n","      <th>2</th>\n","      <th>3</th>\n","      <th>4</th>\n","      <th>5</th>\n","      <th>6</th>\n","      <th>7</th>\n","      <th>8</th>\n","      <th>9</th>\n","      <th>...</th>\n","      <th>90</th>\n","      <th>91</th>\n","      <th>92</th>\n","      <th>93</th>\n","      <th>94</th>\n","      <th>95</th>\n","      <th>96</th>\n","      <th>97</th>\n","      <th>98</th>\n","      <th>99</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>system</th>\n","      <td>-0.000536</td>\n","      <td>0.000236</td>\n","      <td>0.005103</td>\n","      <td>0.009009</td>\n","      <td>-0.009303</td>\n","      <td>-0.007117</td>\n","      <td>0.006459</td>\n","      <td>0.008973</td>\n","      <td>-0.005015</td>\n","      <td>-0.003763</td>\n","      <td>...</td>\n","      <td>0.001631</td>\n","      <td>0.000190</td>\n","      <td>0.003474</td>\n","      <td>0.000218</td>\n","      <td>0.009619</td>\n","      <td>0.005061</td>\n","      <td>-0.008917</td>\n","      <td>-0.007042</td>\n","      <td>0.000901</td>\n","      <td>0.006393</td>\n","    </tr>\n","    <tr>\n","      <th>graph</th>\n","      <td>-0.008620</td>\n","      <td>0.003666</td>\n","      <td>0.005190</td>\n","      <td>0.005742</td>\n","      <td>0.007467</td>\n","      <td>-0.006168</td>\n","      <td>0.001106</td>\n","      <td>0.006047</td>\n","      <td>-0.002840</td>\n","      <td>-0.006174</td>\n","      <td>...</td>\n","      <td>0.001088</td>\n","      <td>-0.001576</td>\n","      <td>0.002197</td>\n","      <td>-0.007882</td>\n","      <td>-0.002717</td>\n","      <td>0.002663</td>\n","      <td>0.005347</td>\n","      <td>-0.002392</td>\n","      <td>-0.009510</td>\n","      <td>0.004506</td>\n","    </tr>\n","    <tr>\n","      <th>trees</th>\n","      <td>0.000095</td>\n","      <td>0.003077</td>\n","      <td>-0.006813</td>\n","      <td>-0.001375</td>\n","      <td>0.007669</td>\n","      <td>0.007346</td>\n","      <td>-0.003673</td>\n","      <td>0.002643</td>\n","      <td>-0.008317</td>\n","      <td>0.006205</td>\n","      <td>...</td>\n","      <td>-0.004509</td>\n","      <td>0.005702</td>\n","      <td>0.009180</td>\n","      <td>-0.004100</td>\n","      <td>0.007965</td>\n","      <td>0.005375</td>\n","      <td>0.005879</td>\n","      <td>0.000513</td>\n","      <td>0.008213</td>\n","      <td>-0.007019</td>\n","    </tr>\n","    <tr>\n","      <th>user</th>\n","      <td>-0.008243</td>\n","      <td>0.009299</td>\n","      <td>-0.000198</td>\n","      <td>-0.001967</td>\n","      <td>0.004604</td>\n","      <td>-0.004095</td>\n","      <td>0.002743</td>\n","      <td>0.006940</td>\n","      <td>0.006065</td>\n","      <td>-0.007511</td>\n","      <td>...</td>\n","      <td>-0.007426</td>\n","      <td>-0.001064</td>\n","      <td>-0.000795</td>\n","      <td>-0.002563</td>\n","      <td>0.009683</td>\n","      <td>-0.000459</td>\n","      <td>0.005874</td>\n","      <td>-0.007448</td>\n","      <td>-0.002506</td>\n","      <td>-0.005550</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>4 rows × 100 columns</p>\n","</div>\n","    <div class=\"colab-df-buttons\">\n","\n","  <div class=\"colab-df-container\">\n","    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-89338bf3-44f0-4f02-a44e-4367b790eb1e')\"\n","            title=\"Convert this dataframe to an interactive table.\"\n","            style=\"display:none;\">\n","\n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n","    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n","  </svg>\n","    </button>\n","\n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    .colab-df-buttons div {\n","      margin-bottom: 4px;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","    <script>\n","      const buttonEl =\n","        document.querySelector('#df-89338bf3-44f0-4f02-a44e-4367b790eb1e button.colab-df-convert');\n","      buttonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","      async function convertToInteractive(key) {\n","        const element = document.querySelector('#df-89338bf3-44f0-4f02-a44e-4367b790eb1e');\n","        const dataTable =\n","          await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                    [key], {});\n","        if (!dataTable) return;\n","\n","        const docLinkHtml = 'Like what you see? Visit the ' +\n","          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","          + ' to learn more about interactive tables.';\n","        element.innerHTML = '';\n","        dataTable['output_type'] = 'display_data';\n","        await google.colab.output.renderOutput(dataTable, element);\n","        const docLink = document.createElement('div');\n","        docLink.innerHTML = docLinkHtml;\n","        element.appendChild(docLink);\n","      }\n","    </script>\n","  </div>\n","\n","\n","<div id=\"df-15324094-164e-4cf5-b645-4b24705944fd\">\n","  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-15324094-164e-4cf5-b645-4b24705944fd')\"\n","            title=\"Suggest charts.\"\n","            style=\"display:none;\">\n","\n","<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","     width=\"24px\">\n","    <g>\n","        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n","    </g>\n","</svg>\n","  </button>\n","\n","<style>\n","  .colab-df-quickchart {\n","      --bg-color: #E8F0FE;\n","      --fill-color: #1967D2;\n","      --hover-bg-color: #E2EBFA;\n","      --hover-fill-color: #174EA6;\n","      --disabled-fill-color: #AAA;\n","      --disabled-bg-color: #DDD;\n","  }\n","\n","  [theme=dark] .colab-df-quickchart {\n","      --bg-color: #3B4455;\n","      --fill-color: #D2E3FC;\n","      --hover-bg-color: #434B5C;\n","      --hover-fill-color: #FFFFFF;\n","      --disabled-bg-color: #3B4455;\n","      --disabled-fill-color: #666;\n","  }\n","\n","  .colab-df-quickchart {\n","    background-color: var(--bg-color);\n","    border: none;\n","    border-radius: 50%;\n","    cursor: pointer;\n","    display: none;\n","    fill: var(--fill-color);\n","    height: 32px;\n","    padding: 0;\n","    width: 32px;\n","  }\n","\n","  .colab-df-quickchart:hover {\n","    background-color: var(--hover-bg-color);\n","    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n","    fill: var(--button-hover-fill-color);\n","  }\n","\n","  .colab-df-quickchart-complete:disabled,\n","  .colab-df-quickchart-complete:disabled:hover {\n","    background-color: var(--disabled-bg-color);\n","    fill: var(--disabled-fill-color);\n","    box-shadow: none;\n","  }\n","\n","  .colab-df-spinner {\n","    border: 2px solid var(--fill-color);\n","    border-color: transparent;\n","    border-bottom-color: var(--fill-color);\n","    animation:\n","      spin 1s steps(1) infinite;\n","  }\n","\n","  @keyframes spin {\n","    0% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","      border-left-color: var(--fill-color);\n","    }\n","    20% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    30% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","      border-right-color: var(--fill-color);\n","    }\n","    40% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    60% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","    }\n","    80% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-bottom-color: var(--fill-color);\n","    }\n","    90% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","    }\n","  }\n","</style>\n","\n","  <script>\n","    async function quickchart(key) {\n","      const quickchartButtonEl =\n","        document.querySelector('#' + key + ' button');\n","      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n","      quickchartButtonEl.classList.add('colab-df-spinner');\n","      try {\n","        const charts = await google.colab.kernel.invokeFunction(\n","            'suggestCharts', [key], {});\n","      } catch (error) {\n","        console.error('Error during call to suggestCharts:', error);\n","      }\n","      quickchartButtonEl.classList.remove('colab-df-spinner');\n","      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n","    }\n","    (() => {\n","      let quickchartButtonEl =\n","        document.querySelector('#df-15324094-164e-4cf5-b645-4b24705944fd button');\n","      quickchartButtonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","    })();\n","  </script>\n","</div>\n","    </div>\n","  </div>\n"],"text/plain":["              0         1         2         3         4         5         6   \\\n","system -0.000536  0.000236  0.005103  0.009009 -0.009303 -0.007117  0.006459   \n","graph  -0.008620  0.003666  0.005190  0.005742  0.007467 -0.006168  0.001106   \n","trees   0.000095  0.003077 -0.006813 -0.001375  0.007669  0.007346 -0.003673   \n","user   -0.008243  0.009299 -0.000198 -0.001967  0.004604 -0.004095  0.002743   \n","\n","              7         8         9   ...        90        91        92  \\\n","system  0.008973 -0.005015 -0.003763  ...  0.001631  0.000190  0.003474   \n","graph   0.006047 -0.002840 -0.006174  ...  0.001088 -0.001576  0.002197   \n","trees   0.002643 -0.008317  0.006205  ... -0.004509  0.005702  0.009180   \n","user    0.006940  0.006065 -0.007511  ... -0.007426 -0.001064 -0.000795   \n","\n","              93        94        95        96        97        98        99  \n","system  0.000218  0.009619  0.005061 -0.008917 -0.007042  0.000901  0.006393  \n","graph  -0.007882 -0.002717  0.002663  0.005347 -0.002392 -0.009510  0.004506  \n","trees  -0.004100  0.007965  0.005375  0.005879  0.000513  0.008213 -0.007019  \n","user   -0.002563  0.009683 -0.000459  0.005874 -0.007448 -0.002506 -0.005550  \n","\n","[4 rows x 100 columns]"]},"execution_count":43,"metadata":{},"output_type":"execute_result"}],"source":["import pandas as pd\n","\n","df = pd.DataFrame(\n","    [embedding.wv.get_vector(word) for word in embedding.wv.key_to_index.keys()],\n","    index=embedding.wv.key_to_index\n","  )\n","\n","df.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":12,"status":"ok","timestamp":1698665704830,"user":{"displayName":"MIRUNA-ANDREEA ZAVELCA","userId":"12692883064275182332"},"user_tz":-120},"id":"EzOAPvYRRBh3","outputId":"e719dac7-cf2c-40c1-cb86-e5bdefdc6e84"},"outputs":[{"data":{"text/plain":["[('graph', -0.01083916611969471),\n"," ('trees', -0.05234673246741295),\n"," ('user', -0.111670583486557)]"]},"execution_count":46,"metadata":{},"output_type":"execute_result"}],"source":["embedding.wv.most_similar('system')"]},{"cell_type":"markdown","metadata":{"id":"TDvBrIhFOC4c"},"source":["### Încărcarea unui model preantrenat\n","<!-- ### Loading a pretrained model -->\n","\n","[Informații despre date și modele](https://github.com/piskvorky/gensim-data)\n","<!-- [Info about data and models](https://github.com/piskvorky/gensim-data) -->\n","\n","[Exemple de utilizare](https://radimrehurek.com/gensim/models/word2vec.html)\n","<!-- [Examples on how to use](https://radimrehurek.com/gensim/models/word2vec.html) -->"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"UgUfTBUF9jhA"},"outputs":[],"source":["import gensim.downloader as api\n","\n","api.info()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/"},"id":"y7RvuW6GWujS","outputId":"dec83c65-c1d9-467c-c884-ee1e20832214"},"outputs":[{"name":"stdout","output_type":"stream","text":["[=================---------------------------------] 35.9% 597.3/1662.8MB downloaded"]}],"source":["model = api.load(\"word2vec-google-news-300\")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":7054,"status":"ok","timestamp":1698666502577,"user":{"displayName":"MIRUNA-ANDREEA ZAVELCA","userId":"12692883064275182332"},"user_tz":-120},"id":"D2TYOua6XTCm","outputId":"d12e4c50-71e2-4f24-8f7b-67d3add58020"},"outputs":[{"data":{"text/plain":["[('systems', 0.7227916717529297),\n"," ('sytem', 0.7129376530647278),\n"," ('sys_tem', 0.5871982574462891),\n"," ('System', 0.5275423526763916),\n"," ('mechanism', 0.5058810114860535),\n"," ('sysem', 0.5027822852134705),\n"," ('systen', 0.49969804286956787),\n"," ('system.The', 0.49599188566207886),\n"," ('sytems', 0.4949610233306885),\n"," ('computerized', 0.47604817152023315)]"]},"execution_count":55,"metadata":{},"output_type":"execute_result"}],"source":["model.most_similar('system')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":31,"status":"ok","timestamp":1698666502578,"user":{"displayName":"MIRUNA-ANDREEA ZAVELCA","userId":"12692883064275182332"},"user_tz":-120},"id":"wARMjmwkXYKL","outputId":"7d009206-3ddd-4556-8439-78b3a6c8f192"},"outputs":[{"data":{"text/plain":["0.09396098"]},"execution_count":56,"metadata":{},"output_type":"execute_result"}],"source":["model.similarity('system', 'graph')"]},{"cell_type":"markdown","metadata":{"id":"JxWxZU_qX19W"},"source":["### Fine-tuning (finisare) pentru modelul anterior:\n","<!-- ### Fine-tuning our model: -->\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"G7SFjytpX46K"},"outputs":[],"source":["model.train(common_texts, total_examples=4, epochs=1)"]},{"cell_type":"markdown","metadata":{"id":"xNpakyqBa_oK"},"source":["Other cool stuff:"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":636,"status":"ok","timestamp":1698666708969,"user":{"displayName":"MIRUNA-ANDREEA ZAVELCA","userId":"12692883064275182332"},"user_tz":-120},"id":"JaJ6FtC0bB1H","outputId":"e19d9f2e-85b2-429b-b05a-a22712159dd3"},"outputs":[{"data":{"text/plain":["[('queen', 0.7118193507194519),\n"," ('monarch', 0.6189674139022827),\n"," ('princess', 0.5902431011199951),\n"," ('crown_prince', 0.5499460697174072),\n"," ('prince', 0.5377321839332581),\n"," ('kings', 0.5236844420433044),\n"," ('Queen_Consort', 0.5235945582389832),\n"," ('queens', 0.5181134343147278),\n"," ('sultan', 0.5098593831062317),\n"," ('monarchy', 0.5087411999702454)]"]},"execution_count":61,"metadata":{},"output_type":"execute_result"}],"source":["model.most_similar(positive=[\"king\", \"woman\"], negative=[\"man\"])"]},{"cell_type":"markdown","metadata":{"id":"p1dZJXZ3bLwb"},"source":["[And less cool stuff:](https://arxiv.org/pdf/1607.06520.pdf)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":670,"status":"ok","timestamp":1698666714419,"user":{"displayName":"MIRUNA-ANDREEA ZAVELCA","userId":"12692883064275182332"},"user_tz":-120},"id":"_iPhuPiVbNUj","outputId":"26be2301-0515-413d-cc4f-2a2ea721462b"},"outputs":[{"data":{"text/plain":["[('homemaker', 0.5627118945121765),\n"," ('housewife', 0.5105047225952148),\n"," ('graphic_designer', 0.505180299282074),\n"," ('schoolteacher', 0.497949481010437),\n"," ('businesswoman', 0.493489146232605),\n"," ('paralegal', 0.49255111813545227),\n"," ('registered_nurse', 0.4907974898815155),\n"," ('saleswoman', 0.4881627559661865),\n"," ('electrical_engineer', 0.4797725975513458),\n"," ('mechanical_engineer', 0.4755399227142334)]"]},"execution_count":62,"metadata":{},"output_type":"execute_result"}],"source":["model.most_similar(positive=[\"computer_programmer\", \"woman\"], negative=[\"man\"])"]},{"cell_type":"markdown","metadata":{"id":"H_dqRxhjbnT_"},"source":["Bias is still an unsolved problem in Machine Learning. Do you know any other popular examples of bias?"]},{"cell_type":"markdown","metadata":{"id":"55UuO7SZRcJs"},"source":["# Principal Component Analysis (PCA) / Analiza componentelor principale\n","\n","PCA este un algoritm de reducere a numărului de dimensiuni -- poate fi util pentru a vizualiza date cu sute de dimensiuni într-un spațiu 2D sau 3D. De exemplu, pentru a observa distanța dintre proiecțiile unor cuvinte, avem:\n","\n","<!-- PCA is a dimensionality reduction algorithm -- meaning that we can use it to visualise our data in 2D or 3D. Here is an example of how you can use it to see the distance between embeddings in 2D: -->"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"iSYbiUpQMaM-"},"outputs":[],"source":["from sklearn.decomposition import PCA\n","\n","text = ['system', 'graph', 'trees', 'user']\n","embeddings = [model[word] for word in text]\n","\n","pca = PCA(n_components=2)\n","pca.fit(embeddings)\n","vectors_2d = pca.transform(embeddings)"]},{"cell_type":"markdown","source":["Interfața din sklearn este asemănătoare ca cea a unui model de ML. După ce antrenăm modelul, putem pune componentele pe un grafic cu matplotlib:\n","\n","<!-- We can train it the same way we would a normal ML model, and visualize the results using, for example, a plotting library like matplotlib: -->"],"metadata":{"id":"K3OgyZ5CB4Wh"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"oxhT5BnPQZXI"},"outputs":[],"source":["import matplotlib.pyplot as plt\n","\n","x = [v[0] for v in vectors_2d]\n","y = [v[1] for v in vectors_2d]\n","\n","fig, ax = plt.subplots()\n","ax.scatter(x, y)\n","\n","for i, txt in enumerate(text):\n","    ax.annotate(txt, (x[i], y[i]))\n","\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"TiHaJDLOgYnp"},"source":["## Global Vectors (GloVe) / Vectori globali\n","\n","Word2Vec se bazează pe statistici locale (apariții la nivel de propoziție). [GloVe](https://nlp.stanford.edu/projects/glove/) ia în calcul statistici globale, ceea ce poate fi util pentru seturi de date mici, nefiind nevoie de multe date de antrenare.\n","\n","Modelul numără toate perechile \"cuvânt1 cuvânt2 ...\" (pentru un context de dimensiune x considerăm cuvinte cu distanța cel mult x între ele) și reține informația într-o matrice de număr de apariții (co-occurrence matrix):\n","\n","<!-- While Word2Vec is based only on local statistics (the occurence of words at\n","a single-sentence level) [GloVe](https://nlp.stanford.edu/projects/glove/) incorporates global statistics methods. This makes it better suited for smaller datasets, as it does not need as much training data. -->\n","\n","<!-- The model counts all \"word1 word2 ...\" pairs (for a context window of x we consider words that have at most distance x between them) and keeps the information in a co-occurrence matrix: -->"]},{"cell_type":"markdown","metadata":{"id":"dJ7WU2agz4b2"},"source":["<center><img src='https://drive.google.com/uc?export=view&id=1pnX1lPdQItUauHp9W8xJlx8q2lgTe4cJ' width=500></center>"]},{"cell_type":"markdown","metadata":{"id":"V6Y4Noij2Egg"},"source":["După aceea, se calculează probabilitatea ca un cuvânt să fie mai aproape de alt cuvânt pe baza acestei matrice:\n","\n","<!-- Afterwards, it computes the probability that a word will be closer to another one based on this matrix: -->\n","$$P(j | i) = \\frac{X_{ij}}{X_i}$$\n","<!-- where: -->\n","unde:\n","$$P(j | i) = \\text{probabilitatea să avem cuvântul j dacă avem cuvântul i}$$\n","$$X_{ij} = \\text{de câte ori apare cuvântul j în contextul cuvântului i}$$\n","$$X_i = \\sum_k X_{ik} = \\text{numărul total de cuvinte care apar în contextul cuvântului i}$$\n","\n","<!-- $$P(j | i) = the\\ probability\\ of\\ word\\ j\\ given\\ i$$\n","$$X_{ij} = how\\ many\\ times\\ word\\ j\\ appears\\ in\\ the\\ context\\ of\\ i$$\n","$$X_i = \\sum_k X_{ik} = sum\\ of\\ how\\ many\\ times\\ words\\ appear\\ in\\ the\\ context\\ of\\ i$$ -->"]},{"cell_type":"markdown","metadata":{"id":"qg2yYtcC_TIJ"},"source":["Pe baza acestor calcule, ar trebui să putem determina relații între cuvinte:\n","\n","<!-- Based on this we should be able to infer relations between words: -->\n","\n","<center><img src='https://nlp.stanford.edu/projects/glove/images/table.png' width=500></center>\n","\n","Observăm că _solid_ are legătură cu _ice_, dar nu și cu _steam_, pe când _gas_ are legătură cu _steam_, dar nu și cu _ice_ (probabilități condiționale foarte mari vs foarte mici). _Water_ și _fasion_ sunt fie strâns corelate cu _ice_ și _steam_ împreună, fie sunt complet fără legătură.\n","\n","<!-- Notice how _solid_ is related to _ice_ but not _steam_, while _gas_ is related to _steam_ but not _ice_ (very large vs. very small conditional values). _Water_ and _fashion_ on the other hand are either highly related to both or completely unrelated. -->"]},{"cell_type":"markdown","metadata":{"id":"oW_-byU0Ad53"},"source":["Mai multe detalii se regăsesc în [articol](https://aclanthology.org/D14-1162.pdf).\n","\n","<!-- Some more computation will bring us to the regression model that is now used for this model. If you want to learn more you can check [the paper](https://aclanthology.org/D14-1162.pdf). -->"]},{"cell_type":"markdown","metadata":{"id":"cFdqo-FRQ7gP"},"source":["### Utilizarea GloVe\n","<!-- ### Using GloVe -->\n","\n","Putem încărca un model GloVe preantrenat folosind biblioteca gensim (sau alte resurse):\n","<!-- We can load a pretrained GloVe model using the gensim library (or other resources): -->"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":243152,"status":"ok","timestamp":1698748384714,"user":{"displayName":"MIRUNA-ANDREEA ZAVELCA","userId":"12692883064275182332"},"user_tz":-120},"id":"0mY_dP3FQ6_n","outputId":"7db963e5-d07a-426a-a131-2ffbfdc9e46d"},"outputs":[{"name":"stdout","output_type":"stream","text":["[==================================================] 100.0% 387.1/387.1MB downloaded\n"]}],"source":["import gensim.downloader as api\n","\n","model = api.load(\"glove-twitter-100\")"]},{"cell_type":"markdown","metadata":{"id":"jUKW1ADEVX5d"},"source":["Pentru a calcula reprezentările cuvintelor (word embeddings) sau pentru alte similarități între cuvinte, la fel ca în cazul Word2Vec:\n","<!-- And use it to compute the word embeddings (or do all other similarity functions that we saw for Word2Vec): -->"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":289,"status":"ok","timestamp":1698749051277,"user":{"displayName":"MIRUNA-ANDREEA ZAVELCA","userId":"12692883064275182332"},"user_tz":-120},"id":"P6ZEmJuKR9S9","outputId":"e937c2a9-78d0-46aa-ea3a-245a9f827a21"},"outputs":[{"data":{"text/plain":["array([ 0.43887 ,  0.32601 , -0.28524 , -0.08248 ,  0.43643 ,  0.75065 ,\n","        0.093945, -0.72626 ,  0.32297 , -0.37128 , -0.23306 ,  0.35499 ,\n","       -3.1764  ,  0.015004,  0.69725 , -0.15256 ,  0.025449, -0.058944,\n","        0.20002 , -0.61298 , -0.79661 ,  0.53051 ,  0.64765 ,  0.90153 ,\n","       -0.27407 ,  0.52871 ,  0.39344 ,  0.56076 ,  0.31942 ,  0.83347 ,\n","       -0.53268 , -1.0166  , -0.25328 , -0.17347 ,  0.68794 ,  0.25902 ,\n","        0.42864 ,  0.3844  , -0.071415, -0.026013, -0.42733 ,  0.58874 ,\n","       -0.30061 , -0.18357 ,  0.21158 , -0.72648 , -0.48477 ,  0.43527 ,\n","       -0.37412 , -0.48493 ,  0.26264 ,  0.21684 , -0.8822  ,  0.57925 ,\n","       -0.54    ,  0.7147  , -0.33133 , -0.44715 , -0.40713 , -0.014364,\n","       -0.083808,  0.45569 , -0.094374,  0.56057 ,  0.65446 , -0.45768 ,\n","        0.2522  ,  0.34328 , -0.061001, -0.4899  ,  0.3342  ,  0.41277 ,\n","       -0.55403 ,  0.30807 ,  0.22867 , -0.53921 ,  0.16439 ,  0.021561,\n","        0.15131 , -0.70287 ,  1.4152  ,  0.83387 ,  0.44385 , -0.042976,\n","        0.069162, -0.74432 , -0.032278, -0.6221  ,  0.20007 ,  0.15834 ,\n","       -0.53907 , -0.31442 ,  0.60969 , -0.32378 ,  0.1676  , -0.94943 ,\n","        0.52916 ,  0.035842, -0.041395, -0.56533 ], dtype=float32)"]},"execution_count":3,"metadata":{},"output_type":"execute_result"}],"source":["model['system']"]},{"cell_type":"markdown","metadata":{"id":"Sc22faRVaCpg"},"source":["Pentru a antrena un model de la zero:\n","<!-- Or you can train your own model from scratch: -->"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kG2UypDfaTjG"},"outputs":[],"source":["from glove import Corpus, Glove\n","\n","corpus = Corpus()\n","corpus.fit(common_texts, window=4)\n","\n","glove = Glove(no_components=4, learning_rate=0.1)\n","glove.fit(corpus.matrix, epochs=10, no_threads=8, verbose=True)\n","glove.add_dictionary(corpus.dictionary)\n","glove.save('glove.model.txt')"]},{"cell_type":"markdown","metadata":{"id":"uidJv81en9Qh"},"source":["# Principal Component Analysis (PCA)\n","\n","PCA is a dimensionality reduction algorithm -- meaning that we can use it to visualise our data in 2D or 3D. Here is an example of how you can use it to see the distance between embeddings in 2D:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_qkjsUVBn9Qi"},"outputs":[],"source":["from sklearn.decomposition import PCA\n","\n","text = ['system', 'graph', 'trees', 'user']\n","embeddings = [model[word] for word in text]\n","\n","pca = PCA(n_components=2)\n","pca.fit(embeddings)\n","vectors_2d = pca.transform(embeddings)"]},{"cell_type":"markdown","source":["We can train it the same way we would a normal ML model, and visualize the results using, for example, a plotting library like matplotlib:"],"metadata":{"id":"p-ftSWuZn9Qi"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"l4Gg4VGSn9Qi"},"outputs":[],"source":["import matplotlib.pyplot as plt\n","\n","x = [v[0] for v in vectors_2d]\n","y = [v[1] for v in vectors_2d]\n","\n","fig, ax = plt.subplots()\n","ax.scatter(x, y)\n","\n","for i, txt in enumerate(text):\n","    ax.annotate(txt, (x[i], y[i]))\n","\n","plt.show()"]},{"cell_type":"markdown","source":["## Exerciții\n","\n","1. Scrie propria implementare de Bag of Words de la zero. Ar trebui să furnizeze atât reprezentări binare, cât și reprezentări pentru numărul de apariții.\n","2. Implementează Tf-Idf de la zero. Definește oricâte funcții ajutătoare crezi necesare.\n","3. Vezi distanța dintre niște cuvinte în 2D folosind PCA (sau altă modalitate de reducere a dimensionalității).\n","4. Creați perechi de (context, cuvânt_țintă) și antrenați o rețea neurală folosind skip-gram sau continuous bag of words. Ar trebui să etichetați fiecare cuvânt cu un ID unic și să folosiți padding pentru antrenarea cuvintelor din capete (context \"dummy\" în stânga/dreapta).\n","5. Vizualizați distanțele dintre câteva cuvinte în 2D folosind PCA (sau alte tehnici de reducere a numărului de dimensiuni).\n","6. Comparați reprezentările de cuvinte în diverse moduri, de exemplu timp de antrenare, cel mai similar cuvânt pentru cuvântul X, distanțe în spațiul 2D, acuratețea cu SVM etc. Comparați implementările voastre cu implementările furnizate de biblioteci."],"metadata":{"id":"gAkdr9isbbmk"}}],"metadata":{"colab":{"collapsed_sections":["sUqzZuWWhUuw","yLcyp5Ed6QKT"],"provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}