{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "gw3eZLKkU-fP",
        "UQMO4b6u007D",
        "izNSmtGue7gD",
        "k---xJS96EJu",
        "91W7wg7b6HY9",
        "zvGoy2ehzcQu"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Preprocesarea Datelor\n",
        "\n",
        "Limbajul scris conține multe elemente care nu relevă informație neapărat relevantă pentru taskul curent. De multe ori este mai bine să eliminăm declinările, punctuația, numerele etc. din modelarea textului. Depinzând de problema curentă, putem alege una sau mai multe metode de curățare a textului:\n",
        "- Tokenizarea textului\n",
        "- Transformarea în litere mici\n",
        "- Eliminarea cifrelor / numerelor sau transformarea lor în cuvinte\n",
        "- Eliminarea linkurilor [LINK]\n",
        "- Eliminarea userilor (@) [USER]\n",
        "- Eliminarea sau înlocuirea hashtagurilor (#)\n",
        "- Eliminarea sau transformarea în cuvinte a emoticoanelor ( :) :D) și a emojiurilor (💙 🐱)\n",
        "- Eliminarea punctuației\n",
        "- Eliminarea cuvintelor comune din limbă (stopwords)\n",
        "- Stemming / lemmatizare"
      ],
      "metadata": {
        "id": "U3sSizRnguC5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# RegEx\n",
        "\n",
        "Un [RegEx](https://www.w3schools.com/python/python_regex.asp) (_Regular Expression_ / _Expresie Regulată_) reprezintă un șir de caractere care definește un șablon de căutare. Poate fi folosit pentru a identifica un subșir într-un string, pentru a-l înlocui sau pentru a împărți textul în jurul lui.\n",
        "\n",
        "Puteți vedea cum funcționează un regex pe un text anume folosind acest link: https://pythex.org/."
      ],
      "metadata": {
        "id": "gw3eZLKkU-fP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "txt = \"The rain   in Spain stays mainly   in the plain\"\n",
        "x = re.search(\"Spai.\", txt)\n",
        "x"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JY3TpT-V5QX3",
        "outputId": "8e694c48-5141-44af-c7e9-fb40430abcbd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<re.Match object; span=(14, 19), match='Spain'>"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Căutarea unui string nu returnează nimic dacă nu găsește niciun match, altfel returnează un obiect cu matchul exact și poziția la care se află. Stringul devine relevant când folosim alte simboluri pentru pattern matching, cum ar fi:\n",
        "- . - orice caracter\n",
        "- \\+ - mai multe apariții ale caracterului anterior\n",
        "- \\* - un număr nedefinit de apariții are caracterului anterior, incluzând 0"
      ],
      "metadata": {
        "id": "6XX0_VEtWJAr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x = re.split(\" +.\", txt)\n",
        "print(x)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "37jcxol7D7yu",
        "outputId": "95b6de60-e634-4cc0-c267-ef854f494185"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['The', 'ain', 'n', 'pain', 'tays', 'ainly', 'n', 'he', 'lain']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Alte caractere speciale:\n",
        "- \\d - cifre\n",
        "- \\D - nu cifre\n",
        "- \\s - spațiu\n",
        "- \\S - nu spațiu\n",
        "- \\w - litere mici, majuscule, caracterul \"_\"\n",
        "- \\W - tot ce nu e \\w\n",
        "- [a-m] - setul de caractere din interior. Poate include intervale\n",
        "\n",
        "Librăria completă poate fi găsită aici https://docs.python.org/3/library/re.html."
      ],
      "metadata": {
        "id": "7c-ppOwgEszZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Putem folosi un regex să identificăm toate cuvintele care includ secvența \"ai\":\n"
      ],
      "metadata": {
        "id": "iZBBAPXQtMAz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x = re.findall(\"\\w*ai\\w\", txt)\n",
        "print(x)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Oxme5CAus_uJ",
        "outputId": "95945a8f-db88-4895-90c1-f2d55e8b850d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['rain', 'Spain', 'main', 'plain']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Tokenizare\n",
        "\n",
        "Tokenizarea este procesul de împărțire a textului în _tokens_. Tokenii nu sunt neapărat cuvinte sau propoziții, ci o secvență de caractere împărțite după o anumită regulă."
      ],
      "metadata": {
        "id": "cbpK2r2z6mgw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Pentru următoarele exerciții vom folosi un corpus din [NLTK](https://www.nltk.org/) pentru analiza sentimentelor."
      ],
      "metadata": {
        "id": "y9ab1d6c0vAR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.corpus import twitter_samples\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('twitter_samples')\n",
        "tweets = twitter_samples.strings('positive_tweets.json')\n",
        "negative_tweets = twitter_samples.strings('negative_tweets.json')\n",
        "tweets"
      ],
      "metadata": {
        "id": "xFT9RrgQ0vAa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk import sent_tokenize, word_tokenize\n",
        "\n",
        "print(tweets[6])\n",
        "print(sent_tokenize(tweets[6]))\n",
        "print(word_tokenize(tweets[6]))"
      ],
      "metadata": {
        "id": "2wMg2p2bFSkF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bb55fafd-9124-47e0-9aa9-343d124f8a95"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "We don't like to keep our lovely customers waiting for long! We hope you enjoy! Happy Friday! - LWWF :) https://t.co/smyYriipxI\n",
            "[\"We don't like to keep our lovely customers waiting for long!\", 'We hope you enjoy!', 'Happy Friday!', '- LWWF :) https://t.co/smyYriipxI']\n",
            "['We', 'do', \"n't\", 'like', 'to', 'keep', 'our', 'lovely', 'customers', 'waiting', 'for', 'long', '!', 'We', 'hope', 'you', 'enjoy', '!', 'Happy', 'Friday', '!', '-', 'LWWF', ':', ')', 'https', ':', '//t.co/smyYriipxI']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Observați cum nltk desparte cuvântul \"don't\". Alte tokenizatoare îl pot păstra întreg. Acesta este un exemplu de decizie de tokenizare care trebuie făcută."
      ],
      "metadata": {
        "id": "X_R5Pa8Q2Rxc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Emoticoane & emojiuri\n",
        "\n",
        "Tokenizarea se bazează de obicei pe spații și punctuație, ceea ce înseamnă că nu știe să gestioneze emoticoanele. O variantă este să ne creăm propriul regex care să identifice simbolurile să să le înlocuiască cu emoția corespunzătoare.\n",
        "\n",
        "Un scurt exemplu:"
      ],
      "metadata": {
        "id": "izNSmtGue7gD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "emoticons = {\n",
        "    \"happy\": r\":[\\)|D+]\",\n",
        "    \"laugh\": r\":\\)\\)+\",\n",
        "    \"sad\": r\":\\(+\"\n",
        "}"
      ],
      "metadata": {
        "id": "JVKGAPTzKrwP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Pentru emoticoane putem folosi biblioteca [emoji](https://pypi.org/project/emoji/):"
      ],
      "metadata": {
        "id": "dL_3jA0j-HrL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install emoji"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_2jGoaJnQUGD",
        "outputId": "435cad4b-5ad3-4e66-8cdb-bf8d562b9f30"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting emoji\n",
            "  Downloading emoji-2.8.0-py2.py3-none-any.whl (358 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/358.9 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m358.4/358.9 kB\u001b[0m \u001b[31m12.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m358.9/358.9 kB\u001b[0m \u001b[31m9.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: emoji\n",
            "Successfully installed emoji-2.8.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import emoji\n",
        "\n",
        "print(tweets[24])\n",
        "emoji.demojize(tweets[24])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "MgVVPDioOeFx",
        "outputId": "0bd32ca5-e94f-447f-fec2-ba56484c5a33"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "💅🏽💋 - :)))) haven't seen you in years\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\":nail_polish_medium_skin_tone::kiss_mark: - :)))) haven't seen you in years\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Lematizare vs. Stemming\n",
        "\n",
        "De cele mai multe ori formatul exact în care apare un text nu este relevant, ci ne interesează mai mult informația pe care o transmite și frecvența sa în text. Din acest motiv putem reduce cuvintele la cea mai simplă formă a lor.\n",
        "\n",
        "În cazul lematizării cea mai simplă formă este forma de dicționar și se numește _lemă_. Stemmingul folosește regex pentru a elimina prefixele și sufixele, ceea ce înseamnă că un _stem_ nu reprezintă neapărat un cuvânt real, sau poate reprezenta un alt cuvânt deja existent.\n",
        "\n",
        "În general folosim stemmingul când vrem un răspuns rapid sau când avem de-a face cu multe cuvinte scrise incorect (ca pe rețelele de socializare)."
      ],
      "metadata": {
        "id": "k---xJS96EJu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![1_HLQgkMt5-g5WO5VpNuTl_g.jpeg](https://miro.medium.com/max/564/1*HLQgkMt5-g5WO5VpNuTl_g.jpeg)"
      ],
      "metadata": {
        "id": "SYFj69rC1T3r"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Aici este un exemplu de cum putem folosi lematizarea sau stemmingul pentru limba engleză:"
      ],
      "metadata": {
        "id": "rzKROdKPfIvC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('wordnet')"
      ],
      "metadata": {
        "id": "uaC3VE7GfxvB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "word = \"leaves\""
      ],
      "metadata": {
        "id": "Z9jeWhD321z2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "print(f\"{word} :\", lemmatizer.lemmatize(word))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YrEn9Ytl1ycZ",
        "outputId": "11133c52-e807-4912-b407-fa4da37cee34"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "leaves : leaf\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import PorterStemmer\n",
        "\n",
        "stemmer = PorterStemmer()\n",
        "\n",
        "print(f\"{word} :\", stemmer.stem(word))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SjH_MhOY2poF",
        "outputId": "22639455-61a8-49a1-9b3b-38922c537be4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "leaves : leav\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Cuvinte comune (Stopwords)\n",
        "\n",
        "Stopwords reprezintă cele mai des utilizate cuvinte dintr-o limbă. Deși au valoare sintactică și morfologică, stopwords nu au foarte multă valoare semantică. Cele mai multe sunt pronume (\"we\"), prepoziții (\"for\") sau conjuncții (\"and\"), dar putem regăsi și verbe (\"is\") sau numerale (\"two\").\n",
        "\n",
        "[Fun fact](https://www.youtube.com/watch?v=fCn8zs912OE)"
      ],
      "metadata": {
        "id": "91W7wg7b6HY9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xZS8k63_rXB0",
        "outputId": "5aae5792-c91d-436c-ed3f-495211f3c4a7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import stopwords\n",
        "\n",
        "stop_words_nltk = set(stopwords.words('english'))\n",
        "print(stop_words_nltk)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U4gju2Q9zUft",
        "outputId": "ce4659c7-1c40-4bee-eca0-3f203443d37f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'through', 'on', 'each', 'out', \"don't\", 'me', 'o', 'in', \"won't\", 'before', \"that'll\", 'm', 'ourselves', \"shouldn't\", 'nor', 'it', \"you'll\", 'has', 'all', 'some', 're', 'to', 'did', 'with', \"wasn't\", 'this', \"isn't\", 'during', 'further', \"haven't\", 'while', 'the', 'over', 'couldn', 'same', 'and', 'have', 'here', \"she's\", 'between', 'aren', \"hasn't\", 'had', 'by', 'other', 'because', \"weren't\", 'than', 't', 'haven', 'until', 'but', 'y', \"couldn't\", 'why', 'do', 've', \"needn't\", 'yourselves', 'having', 's', 'is', 'just', 'isn', 'theirs', 'their', 'myself', 'whom', 'him', 'where', 'yours', 'hasn', 'doing', 'being', \"you'd\", 'they', 'were', 'if', 'shan', 'wasn', 'as', \"should've\", 'll', \"mustn't\", 'off', 'we', 'own', 'under', 'too', \"aren't\", 'shouldn', 'i', 'ma', 'he', 'above', 'once', 'will', 'against', 'his', 'hadn', 'ain', \"didn't\", 'that', 'how', 'there', 'most', 'hers', 'itself', 'our', \"hadn't\", 'then', \"doesn't\", 'd', 'her', 'them', 'be', 'very', 'can', 'she', 'more', 'ours', \"you're\", 'you', 'of', 'your', 'don', 'doesn', 'mightn', 'below', \"wouldn't\", 'both', 'needn', 'should', \"it's\", 'or', 'for', 'won', 'which', 'been', 'so', 'what', 'was', 'only', 'themselves', 'are', 'who', \"mightn't\", 'an', 'again', \"you've\", 'herself', 'few', 'himself', 'when', 'weren', 'its', 'a', 'down', 'not', \"shan't\", 'those', 'no', 'now', 'my', 'these', 'am', 'any', 'after', 'yourself', 'wouldn', 'from', 'about', 'mustn', 'such', 'didn', 'up', 'at', 'into', 'does'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Exerciții\n",
        "\n",
        "\n",
        "\n",
        "1. Scrie o funcție care împarte un text în tokeni folosind regex.\n",
        "2. Scrie o funcție care înlocuiește toate emoticoanele (nu doar cele date ca exemplu) și emojiurile din text.\n",
        "3. Scrie o funcție care primește un text și returnează varianta preprocesată. Funcția va converti toate numerele în cuvinte folosind [num2words](https://pypi.org/project/num2words/), va elimina linkurile, hashtagurile, mentions, punctuația, stopwords, va face toate textele literă mică și va aplica lematizarea sau stemming.\n",
        "4. Analizează setul de date. Uită-te la elemente de preprocesare sau alte features folosind ce ați învățat în primul laborator. Ce pare important?\n",
        "5. Pe baza analizei anterioare scrie o funcție de preprocesare care elimină doar informația irelevantă. Poți generaliza funcția originală specificând în lista de parametri ce modificări vrei să faci la apelarea funcției. Alternativ, poți face o clasă care include o serie de funcții care pot fi alese la inițializare. Cu cât e mai general cu atât mai bine.\n",
        "6. Compară metodele de preprocesare."
      ],
      "metadata": {
        "id": "eJhncK_LaaV8"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "8qAw2Fm344LI"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}