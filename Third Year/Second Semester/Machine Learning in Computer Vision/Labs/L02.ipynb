{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"KPquMWOmYa_D"},"source":["# Antrenarea unei retele de clasificare\n","\n","Obiectivul acestui laborator este de a introduce conceptele de baza necesare antrenarii unei retele neuronale. Pytorch ofera posibilitatea de a incarca si procesa setul de date rapid si eficient. In acest laborator vom folosi setul de date CIFAR-10, pentru care vom rezolva problema de clasificare."]},{"cell_type":"markdown","metadata":{"id":"qzgFBIRQ1YOc"},"source":["Importarea bibliotecilor care vor fi folosite in acest laborator:\n","\n"," * *matplotlib.pyplot* pentru grafice\n"," * *torch.optim* pentru optimizatori\n"," * *torch.nn* pentru lucrul cu retele neurale\n"," * *torch.utils.data* pentru lucrul cu seturi de date\n"," * *torchvision* pentru seturi de date oferite de repository-ul Pytorch"]},{"cell_type":"code","metadata":{"id":"L8uJfA201Bif","executionInfo":{"status":"ok","timestamp":1741523353913,"user_tz":-120,"elapsed":24547,"user":{"displayName":"Adrian Mincu","userId":"17950964671494464787"}}},"source":["from IPython import display as dspl\n","\n","import time\n","\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import torch\n","import torch.optim as optim\n","import torch.nn as nn\n","import torch.utils.data as data\n","import torchvision"],"execution_count":1,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"W7hHS0O7nI4y"},"source":["## Dataset (1p)\n","Clasa *torchvision.datasets.CIFAR10* este o subclasa a clasei abstracte *torch.utils.data.Dataset*. O astfel de clasa este folosita pentru a ingloba datasetul si pentru a returna elemente din dataset.\n","\n","O clasa derivata din *torch.utils.data.Dataset*, trebuie sa suprascrie 2 metode:\n"," * \\_\\_len\\_\\_(self) -> aceasta metoda returneaza numarul de elemente din dataset si permite folosirea functiei __len()__ din Python.\n"," * \\_\\_getitem\\_\\_ -> permite folosirea operatorului de indexare din Python __[ ]__ pentru a obtine un element de la un anumit index din dataset\n","\n","\n","Exemplu clasa derivata din torch.utils.data.Dataset: https://pytorch.org/tutorials/beginner/basics/data_tutorial.html\n","\n","\n","*Obs:* Daca sunt probleme cu descarcarea dataset-ului, poti incerca si asa:\n","\n","`cifar_trainset = datasets.CIFAR10(root='./data', train=True, download=True, transform=None)`\n","\n","`cifar_testset = datasets.CIFAR10(root='./data', train=False, download=True, transform=None)`\n","\n","[CIFAR-10 Google Research Tutorial](https://colab.research.google.com/github/pytorch/tutorials/blob/gh-pages/_downloads/cifar10_tutorial.ipynb)\n","\n","### Cerinte\n","  1. Printati numarul de exemple din datasetul de antrenare si de test (0.25p)\n","  2. Printati exemplul de la indexul 0 (0.25p)\n","  3. Printati valoarea maxima si valoarea minima din prima imagine din datasetul de test (0.5p)\n","\n","#### Hints\n"," * Un exemplu din dataset este reprezentat de un tuplu care contine o imagine de tip de date PIL.Image si un int reprezentand clasa imaginii\n"," * np.min(a) -> returneaza minimul dintr-un obiect de tipul np.ndarray\n"," * np.max(a) -> returneaza maximul dintr-un obiect de tipul np.ndarray\n"," * np.asarray(a) -> returneaza un obiect de tipul np.ndarray. Functia trebuie sa primeaca un obiect 'array-like'"]},{"cell_type":"code","metadata":{"id":"PvoP1C_3-3dA","colab":{"base_uri":"https://localhost:8080/"},"outputId":"961c7fa8-a8b4-43de-b014-7f66e4004820","executionInfo":{"status":"ok","timestamp":1741523611722,"user_tz":-120,"elapsed":1448,"user":{"displayName":"Adrian Mincu","userId":"17950964671494464787"}}},"source":["# Crearea instantelor pentru setul de date CIFAR de train si de test\n","cifar_train = torchvision.datasets.CIFAR10(\"./data\", download=True)\n","cifar_test = torchvision.datasets.CIFAR10(\"./data\", train=False)\n","\n","#TODO: Scrieti aici codul pentru cerinta numarul 1\n","print(f\"Length train: {len(cifar_train)}\\nLength test: {len(cifar_test)}\\n\")\n","\n","#TODO: Scrieti aici codul pentru cerinta numarul 2\n","print(cifar_train[0], end=\"\\n\\n\")\n","\n","#TODO: Completati sub codul pentru cerinta numarul 3\n","print(f\"Max: {np.max(cifar_test[0][0])}\\nMin: {np.min(cifar_test[0][0])}\")"],"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["Files already downloaded and verified\n","Length train: 50000\n","Length test: 10000\n","\n","(<PIL.Image.Image image mode=RGB size=32x32 at 0x7A5C90768BD0>, 6)\n","\n","Max: 255\n","Min: 13\n"]}]},{"cell_type":"markdown","metadata":{"id":"XO0Ju49dp_xp"},"source":["## Iterare prin Dataset (1p)\n","\n","Deoarece clasa datasetului implementeaza functia \\_\\_getitem\\_\\_(), se poate itera prin dataset cum se poate itera si printr-o lista sau alt obiect iterabil.\n","\n","### Cerinte\n"," * Odata la n pasi, printati clasa exemplului curent si afisati imaginea.\n","\n","#### Hints\n","  * A fost importata libraria matplotlib.pyplot as plt\n","  * Functia plt.figure(figsize=(float, float)) returneaza o figura de dimensiunea oferita ca parametru in *figsize*\n","  * Functia plt.imshow(np.ndarray) plaseaza o imagine pe o figura\n","  * Functia plt.show() afiseaza figura"]},{"cell_type":"code","metadata":{"id":"9v_DcyDBHVQu","colab":{"base_uri":"https://localhost:8080/","height":430},"executionInfo":{"status":"ok","timestamp":1741523765608,"user_tz":-120,"elapsed":12467,"user":{"displayName":"Adrian Mincu","userId":"17950964671494464787"}},"outputId":"8069e9d4-0569-435a-ee02-0dd167c2e623"},"source":["n = 10000\n","\n","for idx, example in enumerate(cifar_train):\n","  if idx % n == 0:\n","    # Aceasta functie sterge ce a fost afisat pana la momentul curent\n","    dspl.clear_output(wait=True)\n","\n","    #TODO: Completati codul aici si afisati cateva imagini din dataset\n","    plt.figure(figsize=[6.4, 4.8])\n","    plt.imshow(example[0])\n","    plt.show()\n","\n","    # Aceasta functie opreste procesul pentru 2 secunde\n","    time.sleep(2)"],"execution_count":7,"outputs":[{"output_type":"display_data","data":{"text/plain":["<Figure size 640x480 with 1 Axes>"],"image/png":"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\n"},"metadata":{}}]},{"cell_type":"markdown","metadata":{"id":"Iud_GoZJwK9U"},"source":["## DataLoader (1p)\n","\n","In torch.utils.data este definita clasa *DataLoader*. Aceasta este un wrapper peste o clasa de tip Dataset si este folosita pentru a abstractiza procesarea pe mai mult threaduri, concatenarea exemplelor in batch-uri, si extragerea exemplelor in mod aleatoriu din dataset.\n","\n","Pentru a crea un obiect de tip *DataLoader* se foloseste constructorul care are urmatorul header:\n","\n","  \\_\\_init\\_\\_(dataset_object, batch_size=1, shuffle=False, num_workers=0, collate_fn=None)\n","\n"," * dataset_object - obiectul de tip Dataset care va fi inglobat\n"," * batch_size - dimensiunea batch-ului care va fi returnat\n"," * shuffle - determina daca exemplele vor fi extrase aleatoriu sau nu\n"," * num_workers - numarul de procese paralele care vor incarca datele\n"," * collate_fn - o functie care face preprocesari pe N elemente returnate de obiectul de tip Dataset si le concateneaza intr-un batch. N = batch_size\n","\n","Clasa DataLoader implementeaza si functia *\\_\\_len\\_\\_()* pentru a returna numarul de batch-uri din dataset.\n","\n","Obiectul de tip Dataset creat anterior returneaza un tuple-uri care contin o imagine de tip PIL.Image si clasa imaginii de tip int. O retea neurala din Pytorch opereaza pe tipul de date torch.Tensor. Prin urmare obiectul de tip DataLoader trebuie sa returneze obiecte de tip torch.Tensor\n","\n","### Cerinte\n","  1. Iterati prin cele doua obiecte de tip DataLoader si printati doar primul element. (0.5p)\n","  2. Printati shape-ul celor 2 tensori  doar pentru primul element (0.5p)\n","\n","#### Hints\n","  * functia __to_tensor__ din *torchvision.transforms.functional* creaza un obiect de tip torch.Tensor dintr-un obiect de tip PIL.Image\n","  * functia __torch.tensor__ creaza un obiect de tip torch.Tensor dintr-un obiect de tip np.ndarray\n","  * functia __unsqueeze()__ din clasa torch.Tensor creaza o noua dimensiune intr-un tensor. Aceasta este echivalentul functiei __expand_dims()__ din numpy. Exemplu: Daca avem un obiect de tip torch.Tensor, *t*, care contine un vector cu 10 elemente (shape [10]), *t.unsqueeze(0)* va returna un boiect cu aceleasi valori dar cu shape-ul [1, 10]\n","  * functia __torch.cat(tensors, dim=0)__ primeste o lista de tensori si ii concateneaza de-a lungul dimensiunii *dim*. Exemplu: functia primeste o lista cu doi vectori cu shape-ul [1, 10] si *dim=0*, rezultatul are shape-ul [2, 10] (batch 2 ?). Daca *dim=1*, rezultatul va fi [1, 20]. Echivalentul numpy este functia __concatenate()__\n","  * functia __size()__ din clasa torch.Tensor returneaza shape-ul tensorului"]},{"cell_type":"code","metadata":{"id":"8C_MvX_MIYMv","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1741524595465,"user_tz":-120,"elapsed":17392,"user":{"displayName":"Adrian Mincu","userId":"17950964671494464787"}},"outputId":"36f6a0c1-e612-4213-e4c5-ee446517f239"},"source":["from torchvision.transforms.functional import to_tensor\n","\n","def preproc_fn(examples):\n","  \"\"\"\n","    Functia primeste un batch de exemple pe care trebuie sa le transforme in tensori\n","      si sa le puna intr-un batch de tip torch.Tensor.\n","  \"\"\"\n","  processed_images = []\n","  processed_labels = []\n","\n","  print(processed_images)\n","\n","  for example in examples: # example este un tuplu returnat de obiectul de tip Dataset\n","    pil_image = example[0]\n","    #pil_image_array = np.asarray(pil_image)\n","\n","    tensor_image = to_tensor(pil_image)  # Transforma in obiect de tip torch.Tensor imaginea din example -> 32 x 32 x 3\n","    tensor_image = tensor_image.unsqueeze(0) # Adauga inca o dimensiune la inceputul imaginii -> 1 x 32 x 32 x 3\n","    processed_images.append(tensor_image)\n","\n","    label = np.array([example[1]])# Creaza un obiect de tip np.ndarray din labelul exemplului\n","    tensor_label = torch.Tensor(label)# Creaza un obiect de tip torch.Tensor din label\n","    tensor_label = tensor_label.unsqueeze(0) # Adauga inca o dimensiune la incepului labelului\n","    processed_labels.append(tensor_label)\n","\n","  torch_images = torch.cat(processed_images,  dim=0)\n","  torch_labels = torch.cat(processed_labels, dim=0)\n","\n","  return torch_images, torch_labels\n","\n","loader1 = data.DataLoader(cifar_train, batch_size=1000, shuffle=True, collate_fn=preproc_fn)\n","\n","print(\"Datasetul contine {} de batch-uri\".format(len(loader1)))\n","\n","loader2 = data.DataLoader(cifar_train, batch_size=1000, shuffle=True, num_workers=2, collate_fn=preproc_fn)\n","\n","start = time.time()\n","\n","#TODO: Cerintele 1 si 2 - Iterati prin loader1 si printati doar primul element si shape-ul celor 2 tensori din exemplu\n","for index, batch in enumerate(loader1):\n","  print(batch[0][0][0][0][0], np.shape(batch[0][0]), np.shape(batch[0][1]), end=\" \")\n","\n","end = time.time()\n","print(\"\\nIterarea prin dataset cu worker-ul pe procesul curent dureaza {} secunde\".format(end - start))\n","\n","start = time.time()\n","\n","#TODO: Cerintele 1 si 2 - Iterati prin loader2 si printati doar primul element si shape-ul celor 2 tensori din exemplu\n","for index, batch in enumerate(loader2):\n","  print(batch[0][0][0][0][0], np.shape(batch[0][0]), np.shape(batch[0][1]), end=\" \")\n","\n","end = time.time()\n","print(\"\\nIterarea prin dataset cu 2 worker-i pe procese diferinte dureaza {} secunde\".format(end - start))"],"execution_count":16,"outputs":[{"output_type":"stream","name":"stdout","text":["Datasetul contine 50 de batch-uri\n","[]\n","tensor(0.8549) torch.Size([3, 32, 32]) torch.Size([3, 32, 32]) []\n","tensor(0.9961) torch.Size([3, 32, 32]) torch.Size([3, 32, 32]) []\n","tensor(0.4510) torch.Size([3, 32, 32]) torch.Size([3, 32, 32]) []\n","tensor(0.2745) torch.Size([3, 32, 32]) torch.Size([3, 32, 32]) []\n","tensor(0.1569) torch.Size([3, 32, 32]) torch.Size([3, 32, 32]) []\n","tensor(0.3216) torch.Size([3, 32, 32]) torch.Size([3, 32, 32]) []\n","tensor(0.6667) torch.Size([3, 32, 32]) torch.Size([3, 32, 32]) []\n","tensor(0.6314) torch.Size([3, 32, 32]) torch.Size([3, 32, 32]) []\n","tensor(0.5137) torch.Size([3, 32, 32]) torch.Size([3, 32, 32]) []\n","tensor(0.0275) torch.Size([3, 32, 32]) torch.Size([3, 32, 32]) []\n","tensor(0.8000) torch.Size([3, 32, 32]) torch.Size([3, 32, 32]) []\n","tensor(0.1451) torch.Size([3, 32, 32]) torch.Size([3, 32, 32]) []\n","tensor(0.9529) torch.Size([3, 32, 32]) torch.Size([3, 32, 32]) []\n","tensor(0.5412) torch.Size([3, 32, 32]) torch.Size([3, 32, 32]) []\n","tensor(0.) torch.Size([3, 32, 32]) torch.Size([3, 32, 32]) []\n","tensor(0.2980) torch.Size([3, 32, 32]) torch.Size([3, 32, 32]) []\n","tensor(0.0039) torch.Size([3, 32, 32]) torch.Size([3, 32, 32]) []\n","tensor(0.6902) torch.Size([3, 32, 32]) torch.Size([3, 32, 32]) []\n","tensor(0.6745) torch.Size([3, 32, 32]) torch.Size([3, 32, 32]) []\n","tensor(0.4000) torch.Size([3, 32, 32]) torch.Size([3, 32, 32]) []\n","tensor(0.0980) torch.Size([3, 32, 32]) torch.Size([3, 32, 32]) []\n","tensor(0.2980) torch.Size([3, 32, 32]) torch.Size([3, 32, 32]) []\n","tensor(0.9647) torch.Size([3, 32, 32]) torch.Size([3, 32, 32]) []\n","tensor(0.1882) torch.Size([3, 32, 32]) torch.Size([3, 32, 32]) []\n","tensor(0.9020) torch.Size([3, 32, 32]) torch.Size([3, 32, 32]) []\n","tensor(0.2000) torch.Size([3, 32, 32]) torch.Size([3, 32, 32]) []\n","tensor(0.5569) torch.Size([3, 32, 32]) torch.Size([3, 32, 32]) []\n","tensor(0.3765) torch.Size([3, 32, 32]) torch.Size([3, 32, 32]) []\n","tensor(0.0392) torch.Size([3, 32, 32]) torch.Size([3, 32, 32]) []\n","tensor(0.5098) torch.Size([3, 32, 32]) torch.Size([3, 32, 32]) []\n","tensor(0.4275) torch.Size([3, 32, 32]) torch.Size([3, 32, 32]) []\n","tensor(0.) torch.Size([3, 32, 32]) torch.Size([3, 32, 32]) []\n","tensor(0.4157) torch.Size([3, 32, 32]) torch.Size([3, 32, 32]) []\n","tensor(0.2941) torch.Size([3, 32, 32]) torch.Size([3, 32, 32]) []\n","tensor(0.8471) torch.Size([3, 32, 32]) torch.Size([3, 32, 32]) []\n","tensor(0.1608) torch.Size([3, 32, 32]) torch.Size([3, 32, 32]) []\n","tensor(0.3843) torch.Size([3, 32, 32]) torch.Size([3, 32, 32]) []\n","tensor(0.3216) torch.Size([3, 32, 32]) torch.Size([3, 32, 32]) []\n","tensor(0.6980) torch.Size([3, 32, 32]) torch.Size([3, 32, 32]) []\n","tensor(0.7098) torch.Size([3, 32, 32]) torch.Size([3, 32, 32]) []\n","tensor(0.4235) torch.Size([3, 32, 32]) torch.Size([3, 32, 32]) []\n","tensor(0.9608) torch.Size([3, 32, 32]) torch.Size([3, 32, 32]) []\n","tensor(0.2196) torch.Size([3, 32, 32]) torch.Size([3, 32, 32]) []\n","tensor(0.3647) torch.Size([3, 32, 32]) torch.Size([3, 32, 32]) []\n","tensor(0.9294) torch.Size([3, 32, 32]) torch.Size([3, 32, 32]) []\n","tensor(0.9490) torch.Size([3, 32, 32]) torch.Size([3, 32, 32]) []\n","tensor(0.3529) torch.Size([3, 32, 32]) torch.Size([3, 32, 32]) []\n","tensor(0.9333) torch.Size([3, 32, 32]) torch.Size([3, 32, 32]) []\n","tensor(0.9294) torch.Size([3, 32, 32]) torch.Size([3, 32, 32]) []\n","tensor(0.8549) torch.Size([3, 32, 32]) torch.Size([3, 32, 32]) \n","Iterarea prin dataset cu worker-ul pe procesul curent dureaza 8.35592794418335 secunde\n","[]\n","[]\n","[]\n","[]\n","tensor(0.3490) torch.Size([3, 32, 32]) torch.Size([3, 32, 32]) tensor(0.6745) torch.Size([3, 32, 32]) torch.Size([3, 32, 32]) []\n","[]\n","tensor(0.2118) torch.Size([3, 32, 32]) torch.Size([3, 32, 32]) tensor(0.3373) torch.Size([3, 32, 32]) torch.Size([3, 32, 32]) []\n","[]\n","tensor(0.8667) torch.Size([3, 32, 32]) torch.Size([3, 32, 32]) tensor(0.5608) torch.Size([3, 32, 32]) torch.Size([3, 32, 32]) []\n","[]\n","tensor(0.1020) torch.Size([3, 32, 32]) torch.Size([3, 32, 32]) tensor(0.9608) torch.Size([3, 32, 32]) torch.Size([3, 32, 32]) []\n","[]\n","tensor(0.6000) torch.Size([3, 32, 32]) torch.Size([3, 32, 32]) tensor(1.) torch.Size([3, 32, 32]) torch.Size([3, 32, 32]) []\n","[]\n","tensor(0.4706) torch.Size([3, 32, 32]) torch.Size([3, 32, 32]) tensor(0.4980) torch.Size([3, 32, 32]) torch.Size([3, 32, 32]) []\n","[]\n","tensor(0.4784) torch.Size([3, 32, 32]) torch.Size([3, 32, 32]) tensor(0.5647) torch.Size([3, 32, 32]) torch.Size([3, 32, 32]) []\n","[]\n","tensor(0.3176) torch.Size([3, 32, 32]) torch.Size([3, 32, 32]) tensor(0.5843) torch.Size([3, 32, 32]) torch.Size([3, 32, 32]) []\n","[]\n","tensor(0.3843) torch.Size([3, 32, 32]) torch.Size([3, 32, 32]) tensor(1.) torch.Size([3, 32, 32]) torch.Size([3, 32, 32]) []\n","[]\n","tensor(0.6078) torch.Size([3, 32, 32]) torch.Size([3, 32, 32]) tensor(0.5294) torch.Size([3, 32, 32]) torch.Size([3, 32, 32]) []\n","[]\n","tensor(0.0667) torch.Size([3, 32, 32]) torch.Size([3, 32, 32]) tensor(0.5569) torch.Size([3, 32, 32]) torch.Size([3, 32, 32]) []\n","[]\n","tensor(0.2549) torch.Size([3, 32, 32]) torch.Size([3, 32, 32]) tensor(0.0314) torch.Size([3, 32, 32]) torch.Size([3, 32, 32]) []\n","[]\n","tensor(0.5490) torch.Size([3, 32, 32]) torch.Size([3, 32, 32]) tensor(0.4314) torch.Size([3, 32, 32]) torch.Size([3, 32, 32]) []\n","[]\n","tensor(0.0941) torch.Size([3, 32, 32]) torch.Size([3, 32, 32]) tensor(0.) torch.Size([3, 32, 32]) torch.Size([3, 32, 32]) [][]\n","\n","tensor(0.8275) torch.Size([3, 32, 32]) torch.Size([3, 32, 32]) tensor(0.6667) torch.Size([3, 32, 32]) torch.Size([3, 32, 32]) []\n","[]\n","tensor(0.7255) torch.Size([3, 32, 32]) torch.Size([3, 32, 32]) tensor(0.4627) torch.Size([3, 32, 32]) torch.Size([3, 32, 32]) [][]\n","\n","tensor(0.2667) torch.Size([3, 32, 32]) torch.Size([3, 32, 32]) tensor(0.2157) torch.Size([3, 32, 32]) torch.Size([3, 32, 32]) []\n","[]\n","tensor(0.6863) torch.Size([3, 32, 32]) torch.Size([3, 32, 32]) tensor(0.6196) torch.Size([3, 32, 32]) torch.Size([3, 32, 32]) []\n","[]\n","tensor(0.5294) torch.Size([3, 32, 32]) torch.Size([3, 32, 32]) tensor(0.5922) torch.Size([3, 32, 32]) torch.Size([3, 32, 32]) []\n","[]\n","tensor(0.3961) torch.Size([3, 32, 32]) torch.Size([3, 32, 32]) tensor(0.8980) torch.Size([3, 32, 32]) torch.Size([3, 32, 32]) []\n","[]\n","tensor(0.7765) torch.Size([3, 32, 32]) torch.Size([3, 32, 32]) tensor(0.1412) torch.Size([3, 32, 32]) torch.Size([3, 32, 32]) []\n","[]\n","tensor(0.5020) torch.Size([3, 32, 32]) torch.Size([3, 32, 32]) tensor(0.3098) torch.Size([3, 32, 32]) torch.Size([3, 32, 32]) []\n","[]\n","tensor(0.3216) torch.Size([3, 32, 32]) torch.Size([3, 32, 32]) tensor(0.5020) torch.Size([3, 32, 32]) torch.Size([3, 32, 32]) []\n","[]\n","tensor(0.6824) torch.Size([3, 32, 32]) torch.Size([3, 32, 32]) tensor(0.9765) torch.Size([3, 32, 32]) torch.Size([3, 32, 32]) tensor(0.6706) torch.Size([3, 32, 32]) torch.Size([3, 32, 32]) tensor(0.2353) torch.Size([3, 32, 32]) torch.Size([3, 32, 32]) \n","Iterarea prin dataset cu 2 worker-i pe procese diferinte dureaza 9.041168928146362 secunde\n"]}]},{"cell_type":"markdown","metadata":{"id":"oaJ0EJXOIAjR"},"source":["## Definirea unei retele cu un singur layer\n","\n","Modulul *torch.nn* contine clase si functii utilitare pentru crearea retelelor neurale. Pentru a crea o retea neurala se va defini o clasa, *SingleLayerNet* ce mosteneste din clasa *torch.nn.Module*. Clasele din Pytorch ce definesc straturi, functii de activare, si functii cost mostenesc din clasa *torch.nn.Module*. Aceste clase implementeaza metoda **forward()** care este folosita pentru a defini ce se intampla la un forward pass. Aceasta metoda este apelata in metoda __\\_\\_call\\_\\_()__ a clasei.\n","\n","Clasa *Linear* din *torch.nn*, ce mosteneste din *nn.Module* defineste un strat 'fully-connected'. Contructorul primeste 3 parametrii:\n"," * Dimensiunea vectorului de intrare\n"," * Dimensiunea vectorului de iesire\n"," * Daca sa se foloseasca *bias* sau nu\n","\n","Clasa Simgoid din *torch.nn* , ce mosteneste din *nn.Module*, defineste o functie de activare sigmoid.\n","\n","### Cerinte\n","  1. In constructorul clasei *SingleLayerNet* definiti un atribut care sa contina un obiect de tip *nn.Linear*\n","  2. In constructorul clase *SingleLayerNet* definiti un atribut care sa contina un obiect de tip *nn.Sigmoid*\n","  3. In metoda __forward()__ definiti o variabila care sa contina iesirea stratului linear aplicat pe intrarea 'x'.\n","  4. In metoda __forward()__ definiti o variabila care sa contina iesirea functiei de activare sigmoid aplicata pe iesirea stratului linear si returnati aceasta variabila\n","\n","#### Hint\n"," * Imaginea de intrare are dimensiune 32x32x3 (inaltime x latime x canale). Vectorul de intrare in retea va avea dimensiune 3072.\n","\n","#### Atentie\n"," * Dimensiunea de iesire a stratului trebuie sa fie de aceeasi marime cu numarul de clase."]},{"cell_type":"code","metadata":{"id":"WrSKc8_Hplb2","colab":{"base_uri":"https://localhost:8080/"},"outputId":"d1c8f579-c151-44ae-e761-dee95a678285"},"source":["### Exemplu utilizare metoda __call__()\n","class A(object):\n","  def __init__(self):\n","    self.a ='A'\n","\n","  def __call__(self, mesaj):\n","    print(self.a, mesaj)\n","\n","obj = A()\n","obj(\"OK!\")"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["A OK!\n"]}]},{"cell_type":"code","metadata":{"id":"2vL3FhX5Xi2j"},"source":["import torch.nn as nn\n","\n","class SingleLayerNet(nn.Module):\n","\n","  def __init__(self):\n","    super(SingleLayerNet, self).__init__()\n","\n","\n","    self.linear1 = nn.Linear(3072, 10, bias=False)\n","    self.activation1 = nn.Sigmoid()\n","\n","  def forward(self, x: torch.Tensor):\n","    return self.activation1(self.linear1(x))\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"eB7O0mVkG231"},"source":["# Definirea unei retele cu doua straturi (layere). (2p)\n","\n","### Cerinte\n","  1. In constructorul clasei *TwoLayerNet* definiti doua atribute care sa contina doua obiecte de tip *nn.Linear*. (0.5p)\n","  2. In constructorul clasei *TwoLayerNet* definiti un atribut care sa contina un obiect de tip *nn.Sigmoid* (se poate folosi acelasi obiect de tip *nn.Sigmoid* pentru activarea ambelor straturi *nn.Linear*) (0.5p)\n","  3. Implementati metoda **forward()** similar cu exercitul anterior (layer1->activation->layer2->activation). (1p)\n","\n"]},{"cell_type":"code","metadata":{"id":"SVAxeNFXHURH"},"source":["class TwoLayerNet(nn.Module):\n","\n","  def __init__(self):\n","    super(TwoLayerNet, self).__init__()\n","    # Cerinta 1 - completati codul aici\n","    #self.linear1 = ...\n","    #self.linear2 = ...\n","\n","    # Cerinta 2 - completati codul aici\n","    #self.activation1 = ...\n","\n","  def forward(self, x: torch.Tensor):\n","    # Cerinta 3 si 4- completati codul aici\n","\n","    #return output\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"7ZNoELFZrRpC"},"source":["## Preprocesare pentru retele cu straturi 'fully-connected' (2.5 p)\n","\n","Avand o retea cu straturi 'fully-connected' este necesar ca imaginea sa fie redimensionata intr-un vector. Clasa *torch.Tensor* defineste metoda __view()__ care returneaza un tensor redimensionat.\n","\n","### Cerinte\n","  1. Completati functia de mai jos pentru a redimensiona imaginea intr-un vector. (1p)\n","  2. Extrageti un batch din DataLoader-ul de antrenare si printati dimensiunile imaginii. (1.5p)"]},{"cell_type":"code","metadata":{"id":"lxDsD6hSs8zL","colab":{"base_uri":"https://localhost:8080/"},"outputId":"eab8c30a-be8a-4728-816d-ac29327fd766"},"source":["# Varianta numpy\n","img1 = np.random.rand(32, 32, 3)\n","img2 = np.random.rand(32, 32, 3)\n","\n","print(img1.shape, img2.shape)\n","\n","reshaped1 = img1.reshape(-1)\n","reshaped2 = img2.reshape(32*32*3)\n","\n","print(reshaped1.shape, reshaped2.shape)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["(32, 32, 3) (32, 32, 3)\n","(3072,) (3072,)\n"]}]},{"cell_type":"code","metadata":{"id":"ctqQtdWgW-7V"},"source":["from torchvision.transforms.functional import normalize\n","\n","def preproc_liniarized_fn(examples):\n","  processed_images = []\n","  processed_labels = []\n","\n","  for example in examples:\n","    tensor_image = to_tensor(example[0])\n","    # In linia de mai jos imaginea este normalizata astfel incat sa aiba toate valorile in\n","    # [-1, 1] in loc de [0, 255]\n","    normalized_tensor_image = normalize(tensor_image, [0.5, 0.5, 0.5], [0.5, 0.5, 0.5])\n","    #TODO: Cerinta 1 - completati codul aici\n","    #vector_image = ...\n","    vector_image = vector_image.unsqueeze(0)\n","    processed_images.append(vector_image)\n","\n","    label = np.array(example[1])\n","    tensor_label = torch.tensor(label)\n","    tensor_label = tensor_label.unsqueeze(0)\n","    processed_labels.append(tensor_label)\n","\n","  torch_images = torch.cat(processed_images, dim=0)\n","  torch_labels = torch.cat(processed_labels, dim=0)\n","\n","  return torch_images, torch_labels\n","\n","batch_size = 100\n","\n","train_loader = data.DataLoader(cifar_train, batch_size=batch_size, shuffle=True, num_workers=2, collate_fn=preproc_liniarized_fn)\n","test_loader = data.DataLoader(cifar_test, batch_size=1, shuffle=False, collate_fn=preproc_liniarized_fn)\n","\n","#TODO: Cerinta 2 - completati codul aici\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"YzQgrTWBvvPN"},"source":["## Definirea antrenarii\n","\n","Pentru definirea antrenarii avem urmatorii pasi:\n"," * Definirea numarului de epoci (de cate ori parcurgem intregul dataset)\n"," * Definirea obiectului de tip *SingleLayerNet*\n"," * Definirea optimizatorului. Vom folosi Stochastic Gradient Descent (SGD) pentru optimizarea retelei, prin urmare definim un obiect de tip *optim.SGD*. Constructorul acestei clase primeste parametrii pe care trebuie sa-i optimizeze (single_layer_net.parameters()) si rata de invatare (lr=1e-2)\n"," * Definim functia cost de tip *nn.CrossEntropyLoss()*\n"," * Definim functa care parcurge datasetul si antreneaza reteaua."]},{"cell_type":"code","metadata":{"id":"eqAWKYNqZeuS"},"source":["import torch.optim as optim\n","\n","# Definim numarul de epoci\n","epochs = 10\n","\n","# Definim reteaua\n","single_layer_net = SingleLayerNet()\n","\n","# Definim optimizatorul\n","optimizer = optim.SGD(single_layer_net.parameters(), lr=1e-2)\n","# Dupa definirea optimizatorului si dupa fiecare iteratie trebuie apelata functia zero_grad().\n","# Aceasta face toti gradientii zero.\n","optimizer.zero_grad()\n","\n","# Definim functia cost\n","loss_fn = nn.CrossEntropyLoss()\n","\n","def train_fn(epochs: int, train_loader: data.DataLoader, test_loader: data.DataLoader,\n","             net: nn.Module, loss_fn: nn.Module, optimizer: optim.Optimizer):\n","  # Iteram prin numarul de epoci\n","  for e in range(epochs):\n","    # Iteram prin fiecare exemplu din dataset\n","    for images, labels in train_loader:\n","\n","      # Aplicam reteaua neurala pe imaginile de intrare\n","      out = net(images)\n","      # Aplicam functia cost pe iesirea retelei neurale si pe adnotarile imaginilor\n","      loss = loss_fn(out, labels)\n","      # Aplicam algoritmul de back-propagation\n","      loss.backward()\n","      # Facem pasul de optimizare, pentru a aplica gradientii pe parametrii retelei\n","      optimizer.step()\n","      # Apelam functia zero_grad() pentru a uita gradientii de la iteratie curenta\n","      optimizer.zero_grad()\n","\n","    print(\"Loss-ul la finalul epocii {} are valoarea {}\".format(e, loss.item()))\n","\n","    # Calculul acuratetii\n","    count = len(test_loader)\n","    correct = 0\n","\n","    for test_image, test_label in test_loader:\n","      out_class = torch.argmax(net(test_image))\n","      if out_class == test_label:\n","        correct += 1\n","\n","    print(\"Acuratetea la finalul epocii {} este {:.2f}%\".format(e, (correct / count) * batch_size))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"tE-xJ2b52Dw9"},"source":["## Antrenam propria retea"]},{"cell_type":"code","metadata":{"id":"Pwo4dQ9b_AxM"},"source":["train_fn(epochs, train_loader, test_loader, single_layer_net, loss_fn, optimizer)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"JAGFBjaD2Ji6"},"source":["## Definirea unei retele cu 2 straturi si antrenarea ei (2.5 p)\n","\n","### Cerinte\n"," 1. Instantiati un obiect de tip *TwoLayerNet*. (0.5p)\n"," 2. Definiti un optimizator pentru antrenarea acestei retele (1p)\n"," 3. Folositi functia deifnita mai sus pentru a antrena aceasta retea (*train_fn*). (1p)\n","\n","#### Atentie\n"," * Dimensiunea de iesire a primului strat trebuie sa se potriveasca cu dimensiunea de intrare a celui de-al doilea."]},{"cell_type":"code","metadata":{"id":"nZbXpt9KI6yT"},"source":["#TODO: Cerinta 1 - completati codul aici\n","\n","# Instantierea retelei\n","# two_layer_net = ...\n","\n","#TODO: Cerinta 2 - completati codul aici\n","#optimizer2 = ...\n","\n","# Dupa definirea optimizatorului si dupa fiecare iteratie trebuie apelata functia zero_grad().\n","# Aceasta face toti gradientii zero.\n","optimizer2.zero_grad()\n","\n","# Definim functia de cost\n","loss_fn2 = nn.CrossEntropyLoss()\n","\n","#TODO: Cerinta 3 - Antrenati reteaua\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ICxUifGp3PY1"},"source":["## Crearea dinamica a unei retele\n","\n","In  *torch.nn* exista clasa *Sequential* care primeste o lista de straturi si functii de activare in ordinea in care trebuie aplicate, e.g. [linear, sigmoid, linear, sigmoid]. Rezultatul este inlantuirea acestor straturi si functii de activare.\n"]},{"cell_type":"code","metadata":{"id":"vekIJr0pjfcv"},"source":["class Net(nn.Module):\n","\n","  def __init__(self, layer_sizes: list, activation: type):\n","    \"\"\"\n","      Constructor.\n","\n","      :param layer_sizes - Parametru de tip lista care contine dimensiunile fiecarui strat din retea\n","      :param activation - Parametru de tip type. Poate fi nn.Sigmoid, nn.Tanh, nn.ReLU. Adica clasa pentru a instantia mai tarziu\n","    \"\"\"\n","    super(Net, self).__init__()\n","\n","    layers = []\n","\n","    for i in range(0, len(layer_sizes)):\n","      inl, out = layer_sizes[i]\n","      layers.append(nn.Linear(inl, out))\n","      layers.append(activation)\n","\n","    self.net = nn.Sequential(*layers)\n","\n","  def forward(self, x: torch.Tensor):\n","    return self.net(x)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"3RTE0XJg5ppw"},"source":["## [BONUS] Antrenarea retelei cu N straturi (2p)\n","\n","Mai jos functia de antrenare a fost modificata pentru a afisa modificarile parametrilor retelei in timpul antrenarii. Aceasta se obtine prin implementarea functiei __plot_weights()__.\n","\n","### Cerinte\n","\n","  1. Creati un obiect de tipul *Net* (0.25)\n","  2. Creati un optimizator pentru reteaua de tipul *Net* (0.25)\n","  3. Antrenati reteaua folosind functia __plotting_train_fn()__ (0.25)\n","  4. Experimentati cu retele straturi si functii de activare diferite (0.75)\n","  5. Modificati celula de mai jos si scrieti o functie numita __plot_loss()__ pentru a afisa un grafic care arata evolutia rezultatului functiei cost in timp. Faceti acelasi lucru si pentru acuratete. (0.5)"]},{"cell_type":"code","metadata":{"id":"jZ8vr8fjoEs-"},"source":["def plot_weights(net: nn.Module):\n","  named_params = net.named_parameters()\n","  np_params = []\n","  np_param_names = []\n","  for name, param in named_params:\n","    np_params.append(param.clone().detach().view(-1).numpy())\n","    np_param_names.append(name)\n","\n","  fig = plt.figure(figsize=(20, 2.5))\n","\n","  count = len(np_param_names)\n","  for i in range(count):\n","    plt.subplot(1, count, i+1)\n","    plt.hist(np_params[i], bins=25)\n","    plt.title(np_param_names[i])\n","  plt.show()\n","\n","\n","def plotting_train_fn(epochs: int, train_loader: data.DataLoader, test_loader: data.DataLoader,\n","             net: nn.Module, loss_fn: nn.Module, optimizer: optim.Optimizer):\n","  for e in range(epochs):\n","    for images, labels in train_loader:\n","      optimizer.zero_grad()\n","      out = net(images)\n","      loss = loss_fn(out, labels)\n","      loss.backward()\n","      optimizer.step()\n","\n","\n","    print(\"Loss-ul la finalul epocii {} are valoarea {}\".format(e, loss.item()))\n","\n","    count = len(test_loader)\n","    correct = 0\n","\n","    for test_image, test_label in test_loader:\n","      out_class = torch.argmax(net(test_image))\n","      if out_class == test_label:\n","        correct += 1\n","\n","    print(\"Acuratetea la finalul epocii {} este {:.2f}%\".format(e, (correct / count) * 100))\n","    plot_weights(net)\n","\n","# Cerinta 1 - completati codul aici\n","\n","# Cerinta 2 - completati codul aici\n","\n","# Cerinta 3 - completati codul aici\n","\n","# Cerinta 5 - completati codul aici\n"],"execution_count":null,"outputs":[]}]}