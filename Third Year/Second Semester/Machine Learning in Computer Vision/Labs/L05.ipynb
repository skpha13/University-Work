{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g1-9gKF7c1I2"
      },
      "source": [
        "# Laborator 6 - Segmentare Semantica\n",
        "\n",
        "In acest laborator veti implementa si antrena o retea complet convolutionala ([DeepLabV3](https://arxiv.org/pdf/1706.05587.pdf)), al carei rezultat este o imagine (nu doar clasificare). Veti implementa varianta originala, cat si o adaptare bazata pe MoE (Mixture of Experts) si antrenarea retelei pentru a clasifica fiecare pixel din imagine.\n",
        "\n",
        "Veti construi o retea de segmentare semantica pentru a identifica spatiul liber pe drum (veti folosi setul de date [Kitti Road](http://www.cvlibs.net/datasets/kitti/eval_road.php)).\n",
        "\n",
        "## De ce DeepLabV3?\n",
        "\n",
        "DeepLabV3 este o arhitectură de segmentare semantică avansată care îmbunătățește performanța și precizia în comparație cu alte modele, cum ar fi FCN. Acest lucru se datorează mai multor caracteristici unice ale DeepLabV3.\n",
        "\n",
        "În primul rând, DeepLabV3 utilizează o arhitectură encoder-decoder, similară cu FCN, care păstrează informațiile spațiale pe tot parcursul rețelei, permițându-i să funcționeze cu imagini de orice dimensiune. Aceasta este o caracteristică esențială pentru segmentarea semantică, deoarece permite modelului să înțeleagă contextul spațial al obiectelor din imagine.\n",
        "\n",
        "În al doilea rând, DeepLabV3 introduce un modul de atrous spatial pyramid pooling (ASPP) la finalul encoderului. ASPP permite modelului să captureze informații la diferite scale și să gestioneze mai bine obiectele de diferite dimensiuni. Acest lucru este deosebit de util pentru segmentarea semantică, deoarece obiectele dintr-o imagine pot varia semnificativ în dimensiune.\n",
        "\n",
        "În al treilea rând, DeepLabV3 utilizează convoluții atrous (dilate) pentru a mări câmpul de vedere al rețelei fără a crește numărul de parametri sau a calcula costul. Aceasta permite modelului să capteze mai mult context fără a sacrifica eficiența.\n",
        "\n",
        "Aceste caracteristici fac DeepLabV3 un model puternic pentru segmentarea semantică. În plus, DeepLabV3 gestionează eficient tensiunea între semantică și locație: informațiile globale rezolvă problema clasei - “CE”, în timp ce convoluțiile atrous și ASPP permit modelului să captureze mai bine informațiile locale - “UNDE”.\n",
        "\n",
        "\n",
        "![atrous](https://miro.medium.com/v2/resize:fit:640/format:webp/1*-r7CL0AkeO72MIDpjRxfog.png)\n",
        "\n",
        "![deep](https://miro.medium.com/v2/resize:fit:2000/format:webp/1*nFJ_GqK1D3zKCRgtnRfrcw.png)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nE-LtjCBdOnV"
      },
      "outputs": [],
      "source": [
        "!mkdir ./data ./runs ./saved_models\n",
        "# Download and unzip the dataset\n",
        "!wget --progress=bar:force https://s3.eu-central-1.amazonaws.com/avg-kitti/data_road.zip -P ./data\n",
        "!unzip -q ./data/data_road.zip -d ./data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mATRKjc2l3zK"
      },
      "source": [
        "# Pregatirea setului de date\n",
        "\n",
        "In celula de mai jos sunt implementate clasele torch.utils.data.Dataset ce vor fi folosite pentru a itera prin seturile de date de antrenare si de test din Kitti Road Segmentation.\n",
        "\n",
        "De asemenea sunt implementate doua *collate_fn()* una pentru antrenare ce returneaza si imagini si adnotari, si una pentru testare ce returneaza doar imagini.\n",
        "\n",
        "In final sunt instantiate 3 obiecte torch.utils.data.DataLoader: unul pentru antrenare, unul pentru testare si unul pentru vizualizare.\n",
        "\n",
        "## Cerinta\n",
        "Augumentati datele de antrenare, cu tehnici precum random flip si ajustari de luminozitate, contrast, hue etc. [2p]\n",
        "\n",
        "Hints:\n",
        "* asigurati-va ca augumentarile au sens.\n",
        "* asigurati-va ca label-urile raman corecte.\n",
        "* asigurati-va ca operatia pe care o faceti nu modifica permanent imaginea originala.\n",
        "* toate ajustarile trebuie sa difere de la o epoca la alta,a stfel parametri trebui generati random din diverse distributii (uniforma, gaussiana)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AkLBiBP7eE8l"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import glob\n",
        "import torch\n",
        "import cv2\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision.transforms.functional import normalize, to_tensor\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "\n",
        "%matplotlib inline\n",
        "\n",
        "TRAIN_IMAGES_PATH = \"./data/data_road/training/image_2\"\n",
        "TRAIN_LABELS_PATH = \"./data/data_road/training/gt_image_2\"\n",
        "TEST_IMAGES_PATH = \"./data/data_road/testing/image_2\"\n",
        "\n",
        "\n",
        "def collate_fn_train(examples):\n",
        "\n",
        "  images = []\n",
        "  labels = []\n",
        "  for example in examples:\n",
        "    image, label = example\n",
        "    image = to_tensor(image)\n",
        "    image = normalize(image, [0.5, 0.5, 0.5], [0.5, 0.5, 0.5])\n",
        "    image = image.unsqueeze(0)\n",
        "    label = torch.tensor(label).unsqueeze(0)\n",
        "    images.append(image)\n",
        "    labels.append(label)\n",
        "\n",
        "  images_batch = torch.cat(images)\n",
        "  labels_batch = torch.cat(labels)\n",
        "\n",
        "  return images_batch, labels_batch\n",
        "\n",
        "def collate_fn_test(examples):\n",
        "  images = []\n",
        "  for example in examples:\n",
        "    image = example\n",
        "    image = to_tensor(image)\n",
        "    image = normalize(image, [0.5, 0.5, 0.5], [0.5, 0.5, 0.5])\n",
        "    image = image.unsqueeze(0)\n",
        "    images.append(image)\n",
        "\n",
        "  images_batch = torch.cat(images)\n",
        "\n",
        "  return images_batch\n",
        "\n",
        "class KittiImageDataset(Dataset):\n",
        "\n",
        "  def __init__(self, images_root, original_shape=(1242, 375) ,\n",
        "               load_shape=(384, 128)):\n",
        "    self.images_root = images_root\n",
        "    self.original_shape = original_shape\n",
        "    self.load_shape = load_shape\n",
        "    self.images = list(sorted(glob.glob(os.path.join(TRAIN_IMAGES_PATH ,'*'))))\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    image = cv2.imread(self.images[idx])\n",
        "    image = cv2.resize(image, self.load_shape)\n",
        "\n",
        "    return image\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.images)\n",
        "\n",
        "class KittiTrainDataset(KittiImageDataset):\n",
        "\n",
        "    def __init__(self, images_root, labels_root, original_shape=(1242, 375),\n",
        "                 load_shape=(384, 128)):\n",
        "        super(KittiTrainDataset, self).__init__(images_root, original_shape=original_shape,\n",
        "                                                load_shape=load_shape)\n",
        "        self.labels_root = labels_root\n",
        "        self.labels = list(sorted(glob.glob(os.path.join(TRAIN_LABELS_PATH, '*_road_*'))))\n",
        "\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        image = super(KittiTrainDataset, self).__getitem__(idx)\n",
        "        label = cv2.imread(self.labels[idx])\n",
        "        label = cv2.resize(label, self.load_shape, interpolation=cv2.INTER_NEAREST)\n",
        "        new_label = label.sum(axis=2)\n",
        "        new_label = np.where(new_label == 255, 1, 0)\n",
        "\n",
        "        # Codul vostru aici\n",
        "\n",
        "        return image, new_label\n",
        "\n",
        "labeled_dataset = KittiTrainDataset(TRAIN_IMAGES_PATH, TRAIN_LABELS_PATH)\n",
        "test_dataset = KittiImageDataset(TEST_IMAGES_PATH)\n",
        "\n",
        "# Split the labeled_dataset into train and validation sets\n",
        "train_dataset, val_dataset = train_test_split(labeled_dataset, test_size=0.1, random_state=42)\n",
        "\n",
        "# Create data loaders for train and validation sets\n",
        "vis_dataloader = DataLoader(labeled_dataset, shuffle=True)\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=16, shuffle=True, num_workers=2, collate_fn=collate_fn_train)\n",
        "val_dataloader = DataLoader(val_dataset, shuffle=False, num_workers=2, collate_fn=collate_fn_train)\n",
        "test_dataloader = DataLoader(test_dataset, shuffle=False, collate_fn=collate_fn_test)\n",
        "\n",
        "\n",
        "num_samples = 10\n",
        "vis_iter = iter(vis_dataloader)\n",
        "for i in range(num_samples):\n",
        "\n",
        "  image, label = next(vis_iter)\n",
        "  image = image.squeeze(0).numpy()\n",
        "  label = label.squeeze(0).numpy()\n",
        "  plt.subplot(num_samples, 2, 2*i+1)\n",
        "  plt.axis('off')\n",
        "  plt.imshow(image, None)\n",
        "  plt.subplot(num_samples, 2, 2*i+2)\n",
        "  plt.axis('off')\n",
        "  plt.imshow(label[:,:], 'jet')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "07VXQldrmBTJ"
      },
      "source": [
        "# Implementarea Retelei DeepLab\n",
        "\n",
        "\n",
        "In celula urmatoare se gaseste codul partial pentru implementarea retelei DeepLab.\n",
        "\n",
        "Arhitectura contine:\n",
        " * un encoder, bazat pe ResNet 50 sau ResNet 101.\n",
        " * un decoder, bazat pe module ASPP\n",
        "\n",
        " Mai mult, aceasta arhitectura o vom adapta la ideea de MoE (Mixture of Experts).\n",
        "\n",
        "  ![deeplab](https://miro.medium.com/v2/resize:fit:1400/format:webp/1*5uWAA4VEkFane36gUTHdYg.png)\n",
        "\n",
        "\n",
        " Astfel arhitectura se va modifica:\n",
        " * acelasi encoder\n",
        " * o retea (chiar si cu un singur strat de neuroni) care sa ne dea probabilitatile de a folosi un anume expert.\n",
        " * n decodere, n fiind numarul de experti.\n",
        "\n",
        "\n",
        " Pipelineul acestei retele este urmatorul:\n",
        " * imaginea trece prin encoder\n",
        " * imaginea encodata trece prin reteaua de gating, si se optiun cele n probabilitati\n",
        " * imaginea encodata trece prin *fiecare* din cele n decodere si se obtin n imagini\n",
        " * cele n imagini decodate sunt combinate, folosind o medie ponderata, unde ponderile sunt date de reteaua de gating\n",
        "\n",
        " Astfel, intreaga retea decide ce decoder este mai potrivit pentru un anumit scenariu, in functie de inputul dat.\n",
        "\n",
        "  ![gating](https://deepgram.com/_next/image?url=https%3A%2F%2Fwww.datocms-assets.com%2F96965%2F1695407447-image1.png&w=1920&q=75)\n",
        "\n",
        "## Ceinta\n",
        "\n",
        " a) Implementati arhitectura retelei DeepLab [2p].\n",
        "\n",
        " b) Adapatati arhitectura folosind ideea de MoE [3p].\n",
        "\n",
        " c) Implementati un modul ASPP, asa cum este prezentat in diagrama, la rubrica (a). Nu este nevoie sa implementati si partea de image pooling. [bonus: 1p]\n",
        "\n",
        "\n",
        " **Hint:**\n",
        "  * pentru arhitectura DeepLab puteti sa va folositi de module din torchvision.models.segmentation\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I6yI10zSwzod"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class ASPP(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels=256, atrous_rates=[6, 12, 18]):\n",
        "        super(ASPP, self).__init__()\n",
        "\n",
        "        # Codul vostru aici\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        # Codul vostru aici\n",
        "\n",
        "        return None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "npMFACXKumwe"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision.models as models\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class GatedDeepLab(nn.Module):\n",
        "    def __init__(self, num_classes, num_experts=4):\n",
        "        super(GatedDeepLab, self).__init__()\n",
        "        # Codul vostru aici\n",
        "        self.encoder = None\n",
        "        self.decoders = nn.ModuleList([None for _ in range(num_experts)])\n",
        "        self.gating = None\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Codul vostru aici\n",
        "        return None"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "97OtA1py_9Ap"
      },
      "source": [
        "## Cerinta\n",
        "a) Creati o retea de tip U-net cu acelasi scop de segmentare semantica. Reteaua trebuie scris de la zero, neputand importa module intregi din diverse librarii. [bonus: 1p]\n",
        "\n",
        "b) Antrenati reteaua U-net pe acelasi task. [bonus: 1p]\n",
        "\n",
        "  ![unet](https://lmb.informatik.uni-freiburg.de/people/ronneber/u-net/u-net-architecture.png)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PGxzHO4qwzod"
      },
      "outputs": [],
      "source": [
        "class UNet(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels):\n",
        "        super(UNet, self).__init__()\n",
        "\n",
        "        # Codul vostru aici\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        # Codul vostru aici\n",
        "        return None"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2c39_QRSIlh7"
      },
      "source": [
        "# Antrenarea retelei\n",
        "\n",
        "## Cerinta\n",
        "Setati parametri lipsa si faceti cel putin doua antrenari (cu parametri diferiti) folosind DeepLab si versiunea sa gated [2p]\n",
        "\n",
        "In colab, pentru a folosi GPU-ul trebuie sa mutati sesiune pe T4.\n",
        "In rubrica de \"runtime\" -> \"change runtime type\" -> \"T4 GPU\". Nu uitati sa deconectati sesiunea dupa ce ati terminat."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D5lHNlw8Im1P"
      },
      "outputs": [],
      "source": [
        "# Codul vostru aici\n",
        "num_experts = None\n",
        "num_epochs = None\n",
        "lr = None\n",
        "\n",
        "model = GatedDeepLab(2, num_experts).cuda()\n",
        "\n",
        "import torch.optim as optim\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "# Initialize the loss function\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "# Initialize the optimizer\n",
        "optimizer = optim.AdamW(model.parameters(), lr=lr)\n",
        "optimizer.zero_grad()\n",
        "\n",
        "# Lists to store the training and validation losses\n",
        "train_losses = []\n",
        "val_losses = []\n",
        "\n",
        "# Train the network\n",
        "for epoch in range(num_epochs):\n",
        "\n",
        "  train_loss = 0.0\n",
        "  val_loss = 0.0\n",
        "\n",
        "  # Training\n",
        "  model.train()\n",
        "  for idx, (image, label) in enumerate(train_dataloader):\n",
        "    pred = model(image.cuda())\n",
        "    loss = loss_fn(pred, label.cuda())\n",
        "\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    train_loss += loss.item()\n",
        "\n",
        "    if idx % 10 == 0:\n",
        "      print(\"Epoch {} - Iter {} - Loss {}\".format(epoch, idx, loss.item()))\n",
        "\n",
        "  # Validation\n",
        "  model.eval()\n",
        "  with torch.no_grad():\n",
        "    for idx, (image, label) in enumerate(val_dataloader):\n",
        "      pred = model(image.cuda())\n",
        "      loss = loss_fn(pred, label.cuda())\n",
        "      val_loss += loss.item()\n",
        "\n",
        "  # Calculate average losses\n",
        "  train_loss /= len(train_dataloader)\n",
        "  val_loss /= len(val_dataloader)\n",
        "\n",
        "  # Append losses to the lists\n",
        "  train_losses.append(train_loss)\n",
        "  val_losses.append(val_loss)\n",
        "\n",
        "  print(\"Epoch {} - Training Loss: {:.4f} - Validation Loss: {:.4f}\".format(epoch, train_loss, val_loss))\n",
        "\n",
        "# Plot the training and validation losses\n",
        "plt.plot(train_losses, label='Training Loss')\n",
        "plt.plot(val_losses, label='Validation Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BdmW0zpwIvM4"
      },
      "source": [
        "# Rularea retelei pe setul de date de test si vizualizare\n",
        "\n",
        "## Cerinta\n",
        " Procesati iesirea retelei pentru a vizualiza masca de segmentare prezisa ca in celula pentru pregatirea setului de date. [1p]\n",
        "\n",
        "**Hints**\n",
        " * Iesirea va fi un feature map de dimensiunea imaginii de intrare cu 2 canale.\n",
        " * Pentru fiecare pixel canalele trebuiesc transformate intr-o distributie de probabilitati (Softmax)\n",
        " * Transformati intr-o imagine cu un singur canal cu valori 255 pentru clasa 'road' si 0 pentru clasa 'background'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BwS84fjvJE3U"
      },
      "outputs": [],
      "source": [
        "model.eval()\n",
        "\n",
        "num_to_show = 10\n",
        "\n",
        "softmax = nn.Softmax(dim=1)\n",
        "# Ne asiguram ca nu se pot propaga gradienti prin retea\n",
        "with torch.no_grad():\n",
        "  for idx, image in enumerate(test_dataloader):\n",
        "\n",
        "    if idx > num_to_show:\n",
        "      break\n",
        "\n",
        "    logits = model(image.cuda())\n",
        "\n",
        "    # Postprocesarea imaginii pentru a fi vizualizata\n",
        "    image = (image.squeeze(0).clone().cpu().numpy().transpose(1, 2, 0) + 1) / 2.0\n",
        "\n",
        "    #### Condul vostru aici - postprocesarea iesirii retelei pentru a\n",
        "    #### fi vizualizata\n",
        "    pred = None\n",
        "\n",
        "    # Cod de vizualizare\n",
        "    fig = plt.figure()\n",
        "    ax = fig.add_subplot(1, 2, 1)\n",
        "    plt.imshow(image)\n",
        "    ax = fig.add_subplot(1, 2, 2)\n",
        "    plt.imshow(pred)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.18"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}